{"text": "<!DOCTYPE html>\n<html lang=\"fr\">\n<head>\n  <meta charset=\"UTF-8\">\n  <title>ü§ñ Andy ‚Äî Auto-am√©lioration</title>\n  <style>\n    body { font-family: 'Segoe UI', sans-serif; background: #0d1117; color: #f5f5f5; margin: 0; padding: 0; }\n    header { background: linear-gradient(90deg, #ff8800, #ffcc00); color: #000; text-align: center; padding: 1rem; font-weight: bold; }\n    main { padding: 2rem; max-width: 1000px; margin: auto; }\n    textarea { width: 100%; height: 120px; background: #1a1d29; color: #f5f5f5; border: 1px solid #444; border-radius: 8px; padding: .5rem; }\n    button { margin: .3rem; padding: .6rem 1.2rem; background: #ff8800; border: none; color: #000; border-radius: 6px; cursor: pointer; font-weight: bold; }\n    button:hover { background: #ffaa33; }\n    pre { background: #1a1d29; padding: 1rem; border-radius: 8px; color: #00e676; white-space: pre-wrap; overflow-x: auto; }\n    #status { margin-top: 1rem; color: #aaa; font-style: italic; }\n  </style>\n</head>\n<body>\n  <header>üõ†Ô∏è An", "meta": {"source": "code_index", "path": "app/static/self_review.html"}}
{"text": "hite-space: pre-wrap; overflow-x: auto; }\n    #status { margin-top: 1rem; color: #aaa; font-style: italic; }\n  </style>\n</head>\n<body>\n  <header>üõ†Ô∏è Andy ‚Äî Auto-am√©lioration & V√©rification</header>\n  <main>\n    <h2>Objectif</h2>\n    <textarea id=\"objective\" placeholder=\"Ex : Am√©liorer la recherche, acc√©l√©rer l‚Äôanalyse, optimiser la m√©moire...\"></textarea>\n    <div>\n      <button onclick=\"gen()\">üöÄ G√©n√©rer + V√©rifier</button>\n      <button onclick=\"applyPatch()\">üíæ Valider et appliquer</button>\n    </div>\n\n    <h3>üí° Proposition d‚ÄôAndy :</h3>\n    <pre id=\"out\">Aucune proposition pour le moment...</pre>\n    <div id=\"status\"></div>\n  </main>\n\n  <script>\n    async function gen(){\n      const obj = document.getElementById('objective').value.trim();\n      const status = document.getElementById('status');\n      const out = document.getElementById('out');\n      if (!obj) return alert(\"‚ö†Ô∏è Indique un objectif.\");\n\n      status.textContent = \"üß† G√©n√©ration et v√©rification...\";\n      try {\n        cons", "meta": {"source": "code_index", "path": "app/static/self_review.html"}}
{"text": "ut');\n      if (!obj) return alert(\"‚ö†Ô∏è Indique un objectif.\");\n\n      status.textContent = \"üß† G√©n√©ration et v√©rification...\";\n      try {\n        const res = await fetch(`/self_propose?objective=${encodeURIComponent(obj)}`);\n        const data = await res.json();\n        const patch = data.patch || data.detail || \"\";\n        out.textContent = patch;\n\n        await fetch(\"/log_code\", {\n          method: \"POST\",\n          headers: {\"Content-Type\":\"application/json\"},\n          body: JSON.stringify({code: patch, source: \"self_review\", meta: {objective: obj}})\n        });\n\n        status.textContent = \"‚úÖ Proposition g√©n√©r√©e et enregistr√©e.\";\n      } catch (e) {\n        status.textContent = \"‚ùå Erreur : \" + e.message;\n      }\n    }\n\n    async function applyPatch(){\n      const full = document.getElementById('out').textContent;\n      const m = full.match(/```(?:python)?\\s*\\n([\\s\\S]*?)```/m);\n      const code = m ? m[1] : full;\n\n      const file = prompt(\"Nom du fichier √† modifier (ex: app/ser", "meta": {"source": "code_index", "path": "app/static/self_review.html"}}
{"text": "ull.match(/```(?:python)?\\s*\\n([\\s\\S]*?)```/m);\n      const code = m ? m[1] : full;\n\n      const file = prompt(\"Nom du fichier √† modifier (ex: app/services/search.py)\");\n      if (!file) return alert(\"Fichier non sp√©cifi√©\");\n\n      const resp = await fetch(\"/apply_patch\", {\n        method: \"POST\",\n        headers: {\"Content-Type\":\"application/json\"},\n        body: JSON.stringify({file_path: file, new_code: code})\n      });\n      const data = await resp.json();\n      alert(\"‚úÖ \" + data.detail);\n    }\n  </script>\n</body>\n</html>", "meta": {"source": "code_index", "path": "app/static/self_review.html"}}
{"text": "Ôªø<!doctype html>\n<html lang=\"fr\">\n<head>\n  <meta charset=\"utf-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n  <title>Assistant de Dylan ‚Äî Deep Search</title>\n  <script src=\"https://cdn.tailwindcss.com\"></script>\n  <style>\n    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, \"Liberation Mono\", monospace; }\n    .card { border-radius: 1rem; box-shadow: 0 6px 24px rgba(0,0,0,.08); }\n  </style>\n</head>\n<body class=\"bg-gray-50 text-gray-900\">\n  <div class=\"max-w-5xl mx-auto p-6\">\n    <h1 class=\"text-2xl font-semibold mb-4\">üîé Assistant de Dylan ‚Äî Deep Search</h1>\n\n    <div class=\"card bg-white p-4 mb-6\">\n      <div class=\"grid grid-cols-1 md:grid-cols-6 gap-3 items-end\">\n        <div class=\"md:col-span-3\">\n          <label class=\"block text-sm mb-1\">Requ√™te</label>\n          <input id=\"q\" type=\"text\" placeholder=\"ex: tendances IA en entreprise 2025\"\n                 class=\"w-full rounded-lg border px-3 py-2 focus:outline-none focus:r", "meta": {"source": "code_index", "path": "app/static/ui.html"}}
{"text": "q\" type=\"text\" placeholder=\"ex: tendances IA en entreprise 2025\"\n                 class=\"w-full rounded-lg border px-3 py-2 focus:outline-none focus:ring focus:ring-indigo-200\" />\n        </div>\n        <div>\n          <label class=\"block text-sm mb-1\">Max r√©sultats</label>\n          <input id=\"max_results\" type=\"number\" min=\"1\" max=\"200\" value=\"30\"\n                 class=\"w-full rounded-lg border px-3 py-2 focus:outline-none focus:ring focus:ring-indigo-200\" />\n        </div>\n        <div>\n          <label class=\"block text-sm mb-1\">Max/domain</label>\n          <input id=\"max_per_domain\" type=\"number\" min=\"1\" max=\"50\" value=\"3\"\n                 class=\"w-full rounded-lg border px-3 py-2 focus:outline-none focus:ring focus:ring-indigo-200\" />\n        </div>\n        <div>\n          <label class=\"block text-sm mb-1\">Delay/domain (s)</label>\n          <input id=\"delay_per_domain\" type=\"number\" step=\"0.1\" min=\"0\" max=\"10\" value=\"1.2\"\n                 class=\"w-full rounded-lg border px-3 py-", "meta": {"source": "code_index", "path": "app/static/ui.html"}}
{"text": "<input id=\"delay_per_domain\" type=\"number\" step=\"0.1\" min=\"0\" max=\"10\" value=\"1.2\"\n                 class=\"w-full rounded-lg border px-3 py-2 focus:outline-none focus:ring focus:ring-indigo-200\" />\n        </div>\n        <div class=\"flex items-center gap-3\">\n          <label class=\"inline-flex items-center gap-2\">\n            <input id=\"follow\" type=\"checkbox\" class=\"h-4 w-4\" />\n            <span>Suivre liens</span>\n          </label>\n          <label class=\"inline-flex items-center gap-2\">\n            <input id=\"pretty\" type=\"checkbox\" class=\"h-4 w-4\" />\n            <span>JSON joli</span>\n          </label>\n        </div>\n        <div class=\"md:col-span-6 flex gap-3\">\n          <button id=\"go\" class=\"rounded-lg bg-indigo-600 text-white px-4 py-2 hover:bg-indigo-700\">Rechercher</button>\n          <button id=\"clear\" class=\"rounded-lg bg-gray-200 text-gray-800 px-4 py-2 hover:bg-gray-300\">Effacer</button>\n        </div>\n      </div>\n    </div>\n\n    <div id=\"status\" class=\"text-", "meta": {"source": "code_index", "path": "app/static/ui.html"}}
{"text": "d-lg bg-gray-200 text-gray-800 px-4 py-2 hover:bg-gray-300\">Effacer</button>\n        </div>\n      </div>\n    </div>\n\n    <div id=\"status\" class=\"text-sm text-gray-600 mb-3\"></div>\n    <div id=\"results\" class=\"space-y-4\"></div>\n\n    <details class=\"mt-8\">\n      <summary class=\"cursor-pointer text-sm text-gray-600\">Voir la r√©ponse JSON brute</summary>\n      <pre id=\"raw\" class=\"mono text-xs bg-white p-4 rounded-lg overflow-auto mt-2\"></pre>\n    </details>\n  </div>\n\n<script>\nconst $ = (sel) => document.querySelector(sel);\nconst esc = (s) => (s || \"\").toString().replace(/[&<>]/g, c => ({'&':'&amp;','<':'&lt;','>':'&gt;'}[c]) );\n\nasync function run() {\n  const q = $(\"#q\").value.trim();\n  if (!q) { $(\"#status\").textContent = \"Saisis une requ√™te.\"; return; }\n\n  const max_results = +$(\"#max_results\").value || 30;\n  const follow = $(\"#follow\").checked;\n  const max_per_domain = +$(\"#max_per_domain\").value || 3;\n  const delay_per_domain = +$(\"#delay_per_domain\").value || 1.2;\n  const pretty = $(\"", "meta": {"source": "code_index", "path": "app/static/ui.html"}}
{"text": "cked;\n  const max_per_domain = +$(\"#max_per_domain\").value || 3;\n  const delay_per_domain = +$(\"#delay_per_domain\").value || 1.2;\n  const pretty = $(\"#pretty\").checked;\n\n  const params = new URLSearchParams({ q, max_results, follow, max_per_domain, delay_per_domain, pretty, personalize: true });\n  const url = `/deep_search?${params.toString()}`;\n\n  $(\"#status\").textContent = \"Recherche en cours‚Ä¶\";\n  $(\"#results\").innerHTML = \"\";\n  $(\"#raw\").textContent = \"\";\n\n  try {\n    const res = await fetch(url);\n    const txt = await res.text();\n    let data;\n    try { data = JSON.parse(txt); } catch { data = null; }\n    if (data && data.results) {\n      renderCards(data);\n      $(\"#raw\").textContent = JSON.stringify(data, null, 2);\n      $(\"#status\").textContent = `OK ‚Äî ${data.results.length} r√©sultats (follow=${data.meta.follow_performed ? \"oui\" : \"non\"})`;\n    } else {\n      $(\"#raw\").textContent = txt;\n      $(\"#status\").textContent = \"R√©ponse format√©e (pretty)\";\n    }\n  } catch (e) {\n    $(\"#", "meta": {"source": "code_index", "path": "app/static/ui.html"}}
{"text": "\"non\"})`;\n    } else {\n      $(\"#raw\").textContent = txt;\n      $(\"#status\").textContent = \"R√©ponse format√©e (pretty)\";\n    }\n  } catch (e) {\n    $(\"#status\").textContent = \"Erreur: \" + e.message;\n  }\n}\n\nasync function sendFeedback(item, label) {\n  try {\n    await fetch(\"/feedback\", {\n      method: \"POST\",\n      headers: {\"Content-Type\": \"application/json\"},\n      body: JSON.stringify({ url: item.url, domain: item.domain, title: item.title, label })\n    });\n  } catch (e) { console.warn(\"feedback error\", e); }\n}\n\nfunction renderCards(data) {\n  const wrap = $(\"#results\"); wrap.innerHTML = \"\";\n  for (const it of data.results) {\n    const allowed = it.allowed_by_robots === false\n      ? '<span class=\"text-xs bg-red-100 text-red-700 px-2 py-1 rounded-full\">robots.txt: non</span>'\n      : '<span class=\"text-xs bg-green-100 text-green-700 px-2 py-1 rounded-full\">robots.txt: ok</span>';\n\n    const card = document.createElement(\"div\");\n    card.className = \"card bg-white p-4\";\n    card.innerHTM", "meta": {"source": "code_index", "path": "app/static/ui.html"}}
{"text": "y-1 rounded-full\">robots.txt: ok</span>';\n\n    const card = document.createElement(\"div\");\n    card.className = \"card bg-white p-4\";\n    card.innerHTML = `\n      <div class=\"flex flex-wrap items-center gap-2 mb-2\">\n        <a class=\"text-lg font-medium text-indigo-700 hover:underline\" href=\"${esc(it.url)}\" target=\"_blank\" rel=\"noopener\">\n          ${esc(it.title || it.url)}\n        </a>\n        <span class=\"text-xs bg-gray-100 text-gray-700 px-2 py-1 rounded-full\">${esc(it.domain || \"\")}</span>\n        ${allowed}\n      </div>\n      ${it.snippet ? `<p class=\"text-sm text-gray-700 mb-2\">${esc(it.snippet)}</p>` : ``}\n      ${it.extract ? `<details class=\"mt-1\"><summary class=\"text-sm text-gray-600 cursor-pointer\">Extrait</summary><p class=\"text-sm text-gray-800 mt-1\">${esc(it.extract)}</p></details>` : ``}\n      <div class=\"mt-2 flex gap-2\">\n        <button class=\"px-3 py-1 rounded bg-green-100 text-green-800 hover:bg-green-200 text-sm\" data-act=\"like\">üëç Utile</button>\n        <button cla", "meta": {"source": "code_index", "path": "app/static/ui.html"}}
{"text": "<button class=\"px-3 py-1 rounded bg-green-100 text-green-800 hover:bg-green-200 text-sm\" data-act=\"like\">üëç Utile</button>\n        <button class=\"px-3 py-1 rounded bg-red-100 text-red-800 hover:bg-red-200 text-sm\" data-act=\"dislike\">üëé Sans int√©r√™t</button>\n      </div>\n      <div class=\"mt-2 text-xs text-gray-500 mono\">${esc(it.url)}</div>\n    `;\n    card.querySelector('[data-act=\"like\"]').addEventListener(\"click\", async () => { await sendFeedback(it, \"like\"); card.style.outline = \"2px solid #16a34a55\"; });\n    card.querySelector('[data-act=\"dislike\"]').addEventListener(\"click\", async () => { await sendFeedback(it, \"dislike\"); card.style.outline = \"2px solid #dc262655\"; });\n    wrap.appendChild(card);\n  }\n}\n\ndocument.addEventListener(\"DOMContentLoaded\", () => {\n  $(\"#go\").addEventListener(\"click\", run);\n  $(\"#clear\").addEventListener(\"click\", () => { $(\"#q\").value = \"\"; $(\"#results\").innerHTML = \"\"; $(\"#raw\").textContent = \"\"; $(\"#status\").textContent = \"\"; });\n  $(\"#q\").addEve", "meta": {"source": "code_index", "path": "app/static/ui.html"}}
{"text": "ner(\"click\", () => { $(\"#q\").value = \"\"; $(\"#results\").innerHTML = \"\"; $(\"#raw\").textContent = \"\"; $(\"#status\").textContent = \"\"; });\n  $(\"#q\").addEventListener(\"keydown\", (e) => { if (e.key === \"Enter\") run(); });\n});\n</script>\n</body>\n</html>", "meta": {"source": "code_index", "path": "app/static/ui.html"}}
{"text": "Ôªø", "meta": {"source": "code_index", "path": "app/__init__.py"}}
{"text": "Ôªøimport sqlite3\nfrom pathlib import Path\n\n# Nom d'utilisateur\nUSER_NAME = \"Dylan\"\n\n# Chemin de la base SQLite\nDB_PATH = Path(__file__).resolve().parent / \"prefs.db\"\n\n\ndef db():\n    \"\"\"Retourne une connexion SQLite avec Row factory activ√©e.\"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    conn.row_factory = sqlite3.Row\n    return conn\n\n\ndef init_db():\n    \"\"\"Cr√©e les tables n√©cessaires si elles n‚Äôexistent pas.\"\"\"\n    conn = db()\n    cur = conn.cursor()\n\n    # Table feedback (like/dislike)\n    cur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS feedback (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        created_at TEXT NOT NULL,\n        user TEXT NOT NULL,\n        url TEXT NOT NULL,\n        domain TEXT,\n        title TEXT,\n        label TEXT CHECK(label IN ('like','dislike')) NOT NULL\n    );\n    \"\"\")\n\n    # Table des pr√©f√©rences utilisateur\n    cur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS prefs (\n        id INTEGER PRIMARY KEY CHECK (id=1),\n        user TEXT NOT NULL,\n        preferred_domai", "meta": {"source": "code_index", "path": "app/database.py"}}
{"text": "ur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS prefs (\n        id INTEGER PRIMARY KEY CHECK (id=1),\n        user TEXT NOT NULL,\n        preferred_domains TEXT DEFAULT '',\n        blocked_domains TEXT DEFAULT '',\n        preferred_keywords TEXT DEFAULT '',\n        blocked_keywords TEXT DEFAULT '',\n        like_weight REAL DEFAULT 1.0,\n        dislike_weight REAL DEFAULT -1.0,\n        domain_boost REAL DEFAULT 0.6,\n        keyword_boost REAL DEFAULT 0.4,\n        strict_block INTEGER DEFAULT 0\n    );\n    \"\"\")\n\n    # Table d‚Äôhistorique des recherches (pour la nouveaut√©)\n    cur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS history (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        created_at TEXT NOT NULL,\n        user TEXT NOT NULL,\n        query TEXT NOT NULL,\n        url TEXT NOT NULL,\n        domain TEXT\n    );\n    \"\"\")\n\n    # S‚Äôassurer qu‚Äôune ligne prefs existe toujours pour l‚Äôutilisateur principal\n    cur.execute(\"INSERT OR IGNORE INTO prefs (id, user) VALUES (1, ?)\", (USER_NAME,))", "meta": {"source": "code_index", "path": "app/database.py"}}
{"text": "qu‚Äôune ligne prefs existe toujours pour l‚Äôutilisateur principal\n    cur.execute(\"INSERT OR IGNORE INTO prefs (id, user) VALUES (1, ?)\", (USER_NAME,))\n\n    conn.commit()\n    conn.close()\n\n\ndef reset_db(confirm: bool = False):\n    \"\"\"R√©initialise la base (efface tout).\"\"\"\n    if not confirm:\n        print(\"‚ö†Ô∏è  Utilise reset_db(confirm=True) pour confirmer la suppression.\")\n        return\n    conn = db()\n    cur = conn.cursor()\n    cur.execute(\"DROP TABLE IF EXISTS feedback;\")\n    cur.execute(\"DROP TABLE IF EXISTS prefs;\")\n    cur.execute(\"DROP TABLE IF EXISTS history;\")\n    conn.commit()\n    conn.close()\n    print(\"üóëÔ∏è Base supprim√©e, relance init_db() pour la recr√©er.\")\n\n\nif __name__ == \"__main__\":\n    print(\"üîß Initialisation de la base de donn√©es...\")\n    init_db()\n    print(f\"‚úÖ Base SQLite pr√™te : {DB_PATH}\")", "meta": {"source": "code_index", "path": "app/database.py"}}
{"text": "Ôªø# app/main.py\nimport os\nfrom pathlib import Path\nfrom fastapi import FastAPI\nfrom fastapi.responses import HTMLResponse\n\n# --- Import des routeurs ---\nfrom app.routers.deep_search import router as search_router\nfrom app.routers.chat import router as chat_router\nfrom app.routers.self_update_router import router as self_router\nfrom app.routers.code_review import router as code_review_router\nfrom app.routers.trace import router as trace_router\nfrom app.routers.patch_router import router as patch_router\n\n# --- Services ---\nfrom app.services.startup_indexer import startup_ingest_if_changed\nfrom app.services.code_ingest import ingest_codebase\n\n# --- Initialisation m√©moire ---\nprint(\"üß† Initialisation m√©moire codebase...\")\ntry:\n    res = ingest_codebase(\"app\")\n    print(f\"‚úÖ M√©moire charg√©e ({len(res['updated'])} fichiers modifi√©s)\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è √âchec auto-ingestion:\", e)\n\n# --- Application principale ---\napp = FastAPI(title=\"Assistant IA √©volutif (GPU) de Dylan\")\n\n# -", "meta": {"source": "code_index", "path": "app/main.py"}}
{"text": "tion as e:\n    print(\"‚ö†Ô∏è √âchec auto-ingestion:\", e)\n\n# --- Application principale ---\napp = FastAPI(title=\"Assistant IA √©volutif (GPU) de Dylan\")\n\n# --- Inclusion des routeurs ---\napp.include_router(patch_router)\napp.include_router(search_router)\napp.include_router(chat_router)\napp.include_router(self_router)\napp.include_router(code_review_router)\napp.include_router(trace_router)\n\n# --- Auto-ingestion au d√©marrage ---\n@app.on_event(\"startup\")\ndef _auto_ingest_on_start():\n    \"\"\"\n    √Ä chaque d√©marrage, Andy scanne les fichiers et m√©morise les changements.\n    D√©sactiver avec ENV: ANDY_AUTO_INGEST=0\n    \"\"\"\n    if os.getenv(\"ANDY_AUTO_INGEST\", \"1\") != \"1\":\n        print(\"[startup] Auto-ingest d√©sactiv√© (ANDY_AUTO_INGEST=0).\")\n        return\n\n    res = startup_ingest_if_changed(\n        start=\"app\",\n        allow_ext=(\".py\", \".js\", \".ts\", \".html\", \".css\", \".json\", \".md\", \".txt\"),\n        chunk=1000,\n        overlap=150,\n        max_bytes=300_000,\n        verbose=True\n    )\n    print(\"[st", "meta": {"source": "code_index", "path": "app/main.py"}}
{"text": ".html\", \".css\", \".json\", \".md\", \".txt\"),\n        chunk=1000,\n        overlap=150,\n        max_bytes=300_000,\n        verbose=True\n    )\n    print(\"[startup] Ingest:\", res)\n\n# --- Page d'accueil ---\n@app.get(\"/\")\ndef root():\n    return {\n        \"ok\": True,\n        \"msg\": \"Assistant IA (GPU) ‚Äî pr√™t.\",\n        \"routes\": [r.path for r in app.router.routes],\n        \"ui\": [\"/ui\", \"/chat_ui\", \"/profile\", \"/self_review\"]\n    }\n\n# --- Page Self Review (interface d‚Äôam√©lioration) ---\n@app.get(\"/self_review\", response_class=HTMLResponse)\ndef get_self_review():\n    html_path = Path(\"app/static/self_review.html\")\n    if not html_path.exists():\n        return HTMLResponse(\"<h1>‚ùå Fichier self_review.html introuvable</h1>\", status_code=404)\n\n    # üîÑ Forcer √† relire sans cache navigateur\n    html = html_path.read_text(encoding=\"utf8\")\n    html = html.replace(\n        \"</head>\",\n        '<meta http-equiv=\"Cache-Control\" content=\"no-cache, no-store, must-revalidate\" /></head>'\n    )\n    return HTMLRespo", "meta": {"source": "code_index", "path": "app/main.py"}}
{"text": "lace(\n        \"</head>\",\n        '<meta http-equiv=\"Cache-Control\" content=\"no-cache, no-store, must-revalidate\" /></head>'\n    )\n    return HTMLResponse(content=html)", "meta": {"source": "code_index", "path": "app/main.py"}}
{"text": "Ôªøfrom pydantic import BaseModel\nfrom typing import Optional, List, Dict, Any\n\nclass SearchResult(BaseModel):\n    title: Optional[str]\n    url: str\n    snippet: Optional[str]\n    extract: Optional[str] = None\n    allowed_by_robots: Optional[bool] = None\n    domain: Optional[str] = None\n    score: Optional[float] = None  # <-- important\n\nclass DeepSearchOut(BaseModel):\n    query: str\n    results: List[SearchResult]\n    meta: Dict[str, Any]\n\nclass FeedbackIn(BaseModel):\n    url: str\n    domain: Optional[str] = None\n    title: Optional[str] = None\n    label: str  # 'like' | 'dislike'\n\nclass PrefsIn(BaseModel):\n    preferred_domains: Optional[str] = \"\"\n    blocked_domains: Optional[str] = \"\"\n    preferred_keywords: Optional[str] = \"\"\n    blocked_keywords: Optional[str] = \"\"\n    like_weight: Optional[float] = 1.0\n    dislike_weight: Optional[float] = -1.0\n    domain_boost: Optional[float] = 0.6\n    keyword_boost: Optional[float] = 0.4\n    strict_block: Optional[bool] = False", "meta": {"source": "code_index", "path": "app/models.py"}}
{"text": "# app/models/code_change.py\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass CodeChange(BaseModel):\n    line_number: int = Field(..., ge=1, description=\"Num√©ro de ligne (1-based)\")\n    before: str = Field(..., description=\"Texte avant le changement\")\n    after: str = Field(..., description=\"Texte apr√®s (peut √™tre vide pour suppression)\")\n\nclass CodeReport(BaseModel):\n    file_path: str = Field(..., description=\"Chemin relatif du fichier √† modifier (dans le projet)\")\n    changes: List[CodeChange] = Field(..., description=\"Liste ordonn√©e des changements\")\n    objective: str = Field(\"\", description=\"Contexte/objectif du changement (optionnel)\")", "meta": {"source": "code_index", "path": "app/models/code_change.py"}}
{"text": "Ôªø", "meta": {"source": "code_index", "path": "app/routers/__init__.py"}}
{"text": "from fastapi import APIRouter\nfrom fastapi.responses import HTMLResponse\nfrom app.services.chat_engine import chat_with_user\n\nrouter = APIRouter()\n\n@router.post(\"/chat\")\ndef chat_api(message: str):\n    reply = chat_with_user(message)\n    return {\"reply\": reply}\n\n@router.get(\"/chat_ui\", response_class=HTMLResponse)\ndef chat_ui():\n    return \"\"\"\n<!doctype html><html lang=\"fr\"><head>\n<meta charset=\"utf-8\"><script src=\"https://cdn.tailwindcss.com\"></script>\n<title>Assistant IA ‚Äî Chat</title></head>\n<body class=\"bg-gray-50 p-6\">\n<h1 class=\"text-3xl font-bold mb-4\">üí¨ Assistant IA ‚Äî Chat (GPU)</h1>\n<div id=\"log\" class=\"bg-white rounded p-4 shadow mb-3 max-w-3xl h-[50vh] overflow-auto\"></div>\n<div class=\"flex gap-2\">\n  <input id=\"msg\" class=\"border rounded px-3 py-2 w-2/3\" placeholder=\"Parle avec ton assistant\">\n  <button onclick=\"send()\" class=\"bg-indigo-600 text-white px-4 py-2 rounded\">Envoyer</button>\n</div>\n<script>\nconst log = document.getElementById('log');\nfunction add(role, text){\n  c", "meta": {"source": "code_index", "path": "app/routers/chat.py"}}
{"text": "bg-indigo-600 text-white px-4 py-2 rounded\">Envoyer</button>\n</div>\n<script>\nconst log = document.getElementById('log');\nfunction add(role, text){\n  const b = document.createElement('div');\n  b.className = \"mb-2\";\n  b.innerHTML = `<b>${role}:</b> ${text}`;\n  log.appendChild(b); log.scrollTop = log.scrollHeight;\n}\nasync function send(){\n  const i = document.getElementById('msg'); const m = i.value.trim(); if(!m) return;\n  add(\"Toi\", m); i.value = \"\";\n  const res = await fetch(`/chat?message=${encodeURIComponent(m)}`, {method:\"POST\"});\n  const data = await res.json();\n  add(\"Assistant\", data.reply);\n}\n</script>\n</body></html>\n    \"\"\"", "meta": {"source": "code_index", "path": "app/routers/chat.py"}}
{"text": "# app/routers/code_review.py\nfrom fastapi import APIRouter, Query, HTTPException\nfrom fastapi.responses import JSONResponse\nfrom typing import Optional, List, Dict, Any\nfrom app.models.code_change import CodeReport\nfrom app.services.code_io import list_project_files, read_file, safe_path\nfrom app.services.trace_logger import write_code_trace\nfrom app.services.chat_engine import llm_local\nfrom app.services.memory import add_many, chunk_text, search_memory\n\nrouter = APIRouter(tags=[\"code-review\"])\n\n# ----- lecture projet -----\n@router.get(\"/code/files\")\ndef code_files(start: str = \"app\"):\n    try:\n        return {\"items\": list_project_files(start)}\n    except Exception as e:\n        raise HTTPException(400, str(e))\n\n@router.get(\"/code/file\")\ndef code_file(path: str = Query(..., description=\"Chemin relatif (ex: app/services/search.py)\")):\n    try:\n        content = read_file(path)\n        return {\"path\": path, \"content\": content}\n    except Exception as e:\n        raise HTTPException(400,", "meta": {"source": "code_index", "path": "app/routers/code_review.py"}}
{"text": "try:\n        content = read_file(path)\n        return {\"path\": path, \"content\": content}\n    except Exception as e:\n        raise HTTPException(400, str(e))\n\n# ----- journalisation de rapports -----\n@router.post(\"/report_code\")\ndef report_code(report: CodeReport):\n    payload = {\n        \"file_path\": report.file_path,\n        \"objective\": report.objective,\n        \"changes\": [c.model_dump() for c in report.changes],\n    }\n    res = write_code_trace(\n        code=f\"[CODE-REPORT] {payload}\",\n        source=\"code_review/report_code\",\n        meta={\"file\": report.file_path}\n    )\n    return {\"ok\": True, **res}\n\n# ----- proposition de patch par LLM -----\n@router.post(\"/suggest_patch\")\ndef suggest_patch(file_path: str, objective: str, model: Optional[str] = None):\n    try:\n        content = read_file(file_path)\n    except Exception as e:\n        raise HTTPException(400, f\"Lecture impossible: {e}\")\n\n    prompt = (\n        \"Tu es AssistantDylan. Tu vas proposer un patch complet pour le fichi", "meta": {"source": "code_index", "path": "app/routers/code_review.py"}}
{"text": "raise HTTPException(400, f\"Lecture impossible: {e}\")\n\n    prompt = (\n        \"Tu es AssistantDylan. Tu vas proposer un patch complet pour le fichier ci-dessous.\\n\"\n        \"Contraintes:\\n\"\n        \"- Retourne UNIQUEMENT le code final pr√™t √† coller (pas d'explication).\\n\"\n        \"- Conserve l'API/structure existante au maximum.\\n\"\n        f\"- Objectif: {objective}\\n\\n\"\n        \"----- DEBUT FICHIER -----\\n\"\n        f\"{content}\\n\"\n        \"----- FIN FICHIER -----\\n\"\n        \"Propose le fichier corrig√©/complet :\\n\"\n    )\n    patch = llm_local(prompt)\n    res = write_code_trace(\n        code=patch,\n        source=\"code_review/suggest_patch\",\n        meta={\"file\": file_path, \"objective\": objective, \"model\": model or \"default\"}\n    )\n    return {\"ok\": True, \"patch\": patch, **res}\n\n# ======== NOUVEAU : INGESTION DU CODE EN MEMOIRE ========\n\n@router.post(\"/code/ingest\")\ndef code_ingest(\n    start: str = \"app\",\n    allow_ext: str = \".py,.js,.ts,.html,.css,.json,.md,.txt\",\n    chunk: int = 1", "meta": {"source": "code_index", "path": "app/routers/code_review.py"}}
{"text": "@router.post(\"/code/ingest\")\ndef code_ingest(\n    start: str = \"app\",\n    allow_ext: str = \".py,.js,.ts,.html,.css,.json,.md,.txt\",\n    chunk: int = 1000,\n    overlap: int = 150,\n    max_bytes: int = 300_000\n):\n    \"\"\"\n    Lit les fichiers du projet (r√©pertoire 'start'), d√©coupe le contenu en morceaux,\n    et stocke chaque chunk dans la m√©moire s√©mantique avec des m√©tadonn√©es.\n    \"\"\"\n    files = []\n    try:\n        files = list_project_files(start)\n    except Exception as e:\n        raise HTTPException(400, str(e))\n\n    allow = {e.strip().lower() for e in allow_ext.split(\",\") if e.strip()}\n    to_add: List[Dict[str, Any]] = []\n\n    for rel in files:\n        if allow and (safe_path(rel).suffix.lower() not in allow):\n            continue\n        try:\n            txt = read_file(rel, max_bytes=max_bytes)\n        except Exception:\n            continue\n        chunks = chunk_text(txt, chunk=chunk, overlap=overlap)\n        for i, part in enumerate(chunks):\n            to_add.append({", "meta": {"source": "code_index", "path": "app/routers/code_review.py"}}
{"text": "continue\n        chunks = chunk_text(txt, chunk=chunk, overlap=overlap)\n        for i, part in enumerate(chunks):\n            to_add.append({\n                \"text\": part,\n                \"meta\": {\"kind\": \"code\", \"file\": rel, \"chunk\": i, \"total_chunks\": len(chunks)}\n            })\n\n    n = add_many(to_add)\n    return {\"ok\": True, \"indexed_chunks\": n, \"files_scanned\": len(files)}\n\n@router.get(\"/code/search\")\ndef code_search(q: str, k: int = 8):\n    \"\"\"\n    Recherche s√©mantique dans la m√©moire limit√©e aux items de type 'code'.\n    \"\"\"\n    hits = search_memory(q, k=k, filter_meta={\"kind\": \"code\"})\n    # Raccourci d'affichage\n    out = []\n    for h in hits:\n        meta = h.get(\"meta\") or {}\n        out.append({\n            \"score\": round(h[\"score\"], 4),\n            \"file\": meta.get(\"file\"),\n            \"chunk\": meta.get(\"chunk\"),\n            \"preview\": h[\"text\"][:300]\n        })\n    return {\"query\": q, \"results\": out}", "meta": {"source": "code_index", "path": "app/routers/code_review.py"}}
{"text": "Ôªøfrom fastapi import APIRouter, Query\nfrom fastapi.responses import HTMLResponse, JSONResponse\nfrom app.services.search import (\n    hybrid_search, follow_and_build, rerank,\n    record_feedback, load_json, save_json\n)\n\nrouter = APIRouter()\n\n@router.get(\"/deep_search\")\ndef deep_search(\n    q: str = Query(..., min_length=2),\n    max_results: int = 30,\n    follow: bool = False,\n    pretty: bool = False,\n    personalize: bool = True\n):\n    try:\n        raw = hybrid_search(q, max_results=max_results)\n    except Exception as e:\n        return JSONResponse(status_code=502, content={\"error\": \"search_engine_error\", \"detail\": str(e)})\n\n    results_out = follow_and_build(raw, follow=follow)\n    if personalize:\n        results_out = rerank(results_out, query=q)\n\n    payload = {\n        \"query\": q,\n        \"results\": [it.__dict__ for it in results_out],\n        \"meta\": {\"count\": len(results_out), \"follow\": follow, \"personalize\": personalize}\n    }\n    return JSONResponse(payload, media_type=\"applic", "meta": {"source": "code_index", "path": "app/routers/deep_search.py"}}
{"text": "],\n        \"meta\": {\"count\": len(results_out), \"follow\": follow, \"personalize\": personalize}\n    }\n    return JSONResponse(payload, media_type=\"application/json\", status_code=200) if pretty else payload\n\n\n@router.post(\"/feedback\")\ndef feedback(url: str, liked: bool):\n    record_feedback(url, liked)\n    return {\"ok\": True, \"liked\": liked, \"url\": url}\n\n\n@router.get(\"/ui\", response_class=HTMLResponse)\ndef ui():\n    return \"\"\"\n<!doctype html><html lang=\"fr\"><head>\n<meta charset=\"utf-8\"><script src=\"https://cdn.tailwindcss.com\"></script>\n<title>Assistant IA ‚Äî Recherche</title></head>\n<body class=\"bg-gray-50 p-6\">\n<h1 class=\"text-3xl font-bold mb-4\">üîé Assistant IA ‚Äî Recherche (GPU)</h1>\n<div class=\"flex gap-2 mb-4\">\n  <input id=\"q\" class=\"border rounded px-3 py-2 w-1/2\" placeholder=\"Ex : tendances IA 2025\">\n  <label class=\"flex items-center gap-2\"><input id=\"follow\" type=\"checkbox\"> Suivre liens</label>\n  <button onclick=\"run()\" class=\"bg-indigo-600 text-white px-4 py-2 rounded\">Rechercher</", "meta": {"source": "code_index", "path": "app/routers/deep_search.py"}}
{"text": "-2\"><input id=\"follow\" type=\"checkbox\"> Suivre liens</label>\n  <button onclick=\"run()\" class=\"bg-indigo-600 text-white px-4 py-2 rounded\">Rechercher</button>\n</div>\n<div id=\"status\" class=\"text-sm text-gray-600 mb-2\"></div>\n<div id=\"results\" class=\"space-y-4\"></div>\n<script>\nasync function run(){\n  const q = document.getElementById('q').value.trim();\n  const follow = document.getElementById('follow').checked;\n  if(!q){ return; }\n  document.getElementById('status').textContent = \"Recherche en cours...\";\n  const res = await fetch(`/deep_search?q=${encodeURIComponent(q)}&personalize=true&follow=${follow}`);\n  const data = await res.json();\n  const wrap = document.getElementById('results'); wrap.innerHTML=\"\";\n  for(const r of data.results){\n    const d = document.createElement('div'); d.className=\"bg-white shadow p-4 rounded\";\n    d.innerHTML = `\n      <a href=\"${r.url}\" target=\"_blank\" class=\"text-xl text-indigo-700 font-semibold\">${r.title || r.url}</a><br>\n      <small class=\"text-gray-", "meta": {"source": "code_index", "path": "app/routers/deep_search.py"}}
{"text": "= `\n      <a href=\"${r.url}\" target=\"_blank\" class=\"text-xl text-indigo-700 font-semibold\">${r.title || r.url}</a><br>\n      <small class=\"text-gray-500\">${r.domain || \"\"}</small>\n      <p class=\"mt-1 text-gray-700\">${r.snippet || \"\"}</p>\n      <div class=\"flex gap-3 mt-2\">\n        <button onclick=\"fb('${r.url}',true)\" class=\"text-green-600 hover:underline\">üëç int√©ressant</button>\n        <button onclick=\"fb('${r.url}',false)\" class=\"text-red-600 hover:underline\">üëé sans int√©r√™t</button>\n      </div>`;\n    wrap.appendChild(d);\n  }\n  document.getElementById('status').textContent = `${data.results.length} r√©sultats`;\n}\nasync function fb(url, liked){\n  await fetch(`/feedback?url=${encodeURIComponent(url)}&liked=${liked}`, {method:\"POST\"});\n}\n</script>\n</body></html>\n    \"\"\"\n\n\n@router.get(\"/profile\", response_class=HTMLResponse)\ndef profile():\n    data = load_json(\"data/user_prefs.json\")\n    prefs = data.get(\"preferences\", {\"boost\": [], \"ban\": []})\n    hist = data.get(\"history\", [])\n    las", "meta": {"source": "code_index", "path": "app/routers/deep_search.py"}}
{"text": "data = load_json(\"data/user_prefs.json\")\n    prefs = data.get(\"preferences\", {\"boost\": [], \"ban\": []})\n    hist = data.get(\"history\", [])\n    last = \"\".join([f\"<li>{h.get('query','?')} ‚Üí {h.get('domain','')} : {h.get('title','')}</li>\" for h in hist[-10:]])\n    return f\"\"\"\n<html><body style='font-family:Arial;padding:2rem'>\n<h1>üß† Profil IA</h1>\n<h2>Sites favoris (boost)</h2><ul>{''.join(f'<li>{b}</li>' for b in prefs.get('boost', []))}</ul>\n<h2>Sites bannis (ban)</h2><ul>{''.join(f'<li>{b}</li>' for b in prefs.get('ban', []))}</ul>\n<h2>Historique r√©cent</h2><ul>{last or \"<li>(vide)</li>\"}</ul>\n<form method='post' action='/reset'><button type='submit'>üîÑ R√©initialiser</button></form>\n</body></html>\n    \"\"\"\n\n\n@router.post(\"/reset\")\ndef reset():\n    save_json(\"data/user_prefs.json\", {\"preferences\": {\"boost\": [], \"ban\": []}, \"history\": []})\n    return {\"ok\": True, \"msg\": \"R√©initialis√©\"}", "meta": {"source": "code_index", "path": "app/routers/deep_search.py"}}
{"text": "Ôªøfrom fastapi import APIRouter, HTTPException\nfrom datetime import datetime\nfrom ..models import FeedbackIn, PrefsIn\nfrom ..database import db\nfrom ..services.prefs import load_prefs\n\nrouter = APIRouter()\n\nUSER_NAME = \"Dylan\"\n\n@router.post(\"/feedback\")\ndef post_feedback(fb: FeedbackIn):\n    label = (fb.label or \"\").lower()\n    if label not in (\"like\", \"dislike\"):\n        raise HTTPException(400, \"label doit √™tre 'like' ou 'dislike'\")\n    conn = db()\n    conn.execute(\n        \"INSERT INTO feedback (created_at, user, url, domain, title, label) VALUES (?, ?, ?, ?, ?, ?)\",\n        (datetime.utcnow().isoformat(), USER_NAME, fb.url, (fb.domain or \"\"), (fb.title or \"\"), label)\n    )\n    conn.commit()\n    conn.close()\n    return {\"ok\": True}\n\n@router.get(\"/prefs\")\ndef get_prefs():\n    return load_prefs()\n\n@router.post(\"/prefs\")\ndef set_prefs(p: PrefsIn):\n    conn = db()\n    conn.execute(\"\"\"\n      UPDATE prefs SET\n        preferred_domains=?, blocked_domains=?, preferred_keywords=?, blocked_key", "meta": {"source": "code_index", "path": "app/routers/feedback.py"}}
{"text": "refsIn):\n    conn = db()\n    conn.execute(\"\"\"\n      UPDATE prefs SET\n        preferred_domains=?, blocked_domains=?, preferred_keywords=?, blocked_keywords=?,\n        like_weight=?, dislike_weight=?, domain_boost=?, keyword_boost=?, strict_block=?\n      WHERE id=1\n    \"\"\", (\n        p.preferred_domains or \"\",\n        p.blocked_domains or \"\",\n        p.preferred_keywords or \"\",\n        p.blocked_keywords or \"\",\n        float(p.like_weight or 1.0),\n        float(p.dislike_weight or -1.0),\n        float(p.domain_boost or 0.6),\n        float(p.keyword_boost or 0.4),\n        1 if (getattr(p, \"strict_block\", False) is True) else 0\n    ))\n    conn.commit()\n    conn.close()\n    return {\"ok\": True, \"prefs\": load_prefs()}", "meta": {"source": "code_index", "path": "app/routers/feedback.py"}}
{"text": "from fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel\nfrom pathlib import Path\n\nrouter = APIRouter(tags=[\"patch\"])\n\nclass PatchIn(BaseModel):\n    file_path: str\n    new_code: str\n\n@router.post(\"/apply_patch\")\ndef apply_patch(patch: PatchIn):\n    path = Path(patch.file_path)\n    if not path.exists():\n        raise HTTPException(status_code=404, detail=\"Fichier introuvable\")\n\n    backup_path = path.with_suffix(\".bak\")\n    path.rename(backup_path)  # Sauvegarde de s√©curit√©\n\n    with open(path, \"w\", encoding=\"utf8\") as f:\n        f.write(patch.new_code)\n\n    return {\"detail\": f\"Code mis √† jour avec succ√®s dans {path.name}\"}", "meta": {"source": "code_index", "path": "app/routers/patch_router.py"}}
{"text": "# app/routers/self_update_router.py\nfrom typing import Optional\nfrom fastapi import APIRouter, Query\nfrom fastapi.responses import HTMLResponse, JSONResponse\nfrom pydantic import BaseModel, Field\n\nfrom app.services.self_update import propose_code_improvement, add_note, list_notes\nfrom app.services.verification import verify_patch\nfrom app.services.trace_logger import write_code_trace  # utilis√© par /log_code si tu l'as\n\nrouter = APIRouter(tags=[\"self-update\"])\n\n# =========================================================\n# ‚ö†Ô∏è IMPORTANT :\n# Si tu gardes cette route /self_review ici,\n# supprime/renomme la route /self_review dans app/main.py\n# pour √©viter le conflit.\n# =========================================================\n\n@router.get(\"/self_review\", response_class=HTMLResponse)\ndef self_review_ui() -> str:\n    try:\n        notes = list_notes()\n    except Exception:\n        notes = []\n\n    items = \"\".join(f\"<li>{n}</li>\" for n in notes) or \"<li>(aucune note)</li>\"\n\n    html = \"\"\"\n<!doc", "meta": {"source": "code_index", "path": "app/routers/self_update_router.py"}}
{"text": "otes()\n    except Exception:\n        notes = []\n\n    items = \"\".join(f\"<li>{n}</li>\" for n in notes) or \"<li>(aucune note)</li>\"\n\n    html = \"\"\"\n<!doctype html>\n<html>\n<head>\n  <meta charset=\"utf-8\" />\n  <title>Self Review</title>\n  <style>\n    body { font-family: Arial, system-ui, -apple-system, Segoe UI, Roboto, sans-serif; padding: 2rem; max-width: 1100px; margin: 0 auto; background:#0d1117; color:#f5f5f5; }\n    textarea { width: 100%; max-width: 1100px; background:#151a24; color:#f5f5f5; border:1px solid #333; border-radius:8px; }\n    pre { white-space: pre-wrap; background: #151a24; padding: 1rem; border-radius: 8px; max-width: 1100px; overflow:auto; color:#00e676; }\n    button { padding: .6rem 1rem; border-radius: .5rem; background: #ff8800; color: #000; border: 0; cursor: pointer; font-weight:600; }\n    button:hover { background: #ffaa33; }\n    .muted { color: #aaa; font-size: .95rem; }\n    .status { margin-top: .5rem; font-size: .9rem; color: #ccc; }\n    .ok { color: #00bd74; }", "meta": {"source": "code_index", "path": "app/routers/self_update_router.py"}}
{"text": "ffaa33; }\n    .muted { color: #aaa; font-size: .95rem; }\n    .status { margin-top: .5rem; font-size: .9rem; color: #ccc; }\n    .ok { color: #00bd74; }\n    .err { color: #ff6b6b; }\n    .card { background: #0f1420; border-radius: 10px; padding: 1rem; box-shadow: 0 6px 24px rgba(0,0,0,.25); margin-top: 1rem; }\n    .grid { display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; }\n    ul { padding-left: 1.2rem; }\n    a { color:#7aa2ff; }\n  </style>\n</head>\n<body>\n  <h1>üõ†Ô∏è Auto-am√©lioration (avec v√©rification)</h1>\n\n  <form onsubmit=\"event.preventDefault(); gen();\">\n    <p class=\"muted\">Objectif (ex: ‚Äúacc√©l√©rer la recherche‚Äù, ‚Äúmieux classer les r√©sultats‚Äù, etc.)</p>\n    <textarea id=\"objective\" rows=\"4\" placeholder=\"D√©cris l'objectif d'am√©lioration...\"></textarea><br /><br />\n    <button type=\"submit\">üöÄ G√©n√©rer + V√©rifier</button>\n    <button type=\"button\" onclick=\"applyPatch()\">üíæ Valider et appliquer</button>\n    <div id=\"status\" class=\"status\"></div>\n  </form>\n\n  <div class=\"grid\">\n    <", "meta": {"source": "code_index", "path": "app/routers/self_update_router.py"}}
{"text": "type=\"button\" onclick=\"applyPatch()\">üíæ Valider et appliquer</button>\n    <div id=\"status\" class=\"status\"></div>\n  </form>\n\n  <div class=\"grid\">\n    <div class=\"card\">\n      <h2>Proposition d'Andy</h2>\n      <pre id=\"out\"></pre>\n    </div>\n    <div class=\"card\">\n      <h2>Verdict</h2>\n      <pre id=\"verdict\"></pre>\n    </div>\n  </div>\n\n  <h2>üìù Notes</h2>\n  <ul>__NOTES__</ul>\n\n  <script>\n    async function gen(){\n      const obj = document.getElementById('objective').value || \"\";\n      const statusEl = document.getElementById('status');\n      const outEl = document.getElementById('out');\n      const verEl = document.getElementById('verdict');\n\n      outEl.textContent = \"\";\n      verEl.textContent = \"\";\n      statusEl.textContent = \"‚è≥ G√©n√©ration en cours...\";\n\n      try {\n        // 1) G√©n√©ration du patch\n        const res = await fetch(`/self_propose?objective=${encodeURIComponent(obj)}`);\n        const data = await res.json();\n        const patch = (data && typeof data.patch === \"strin", "meta": {"source": "code_index", "path": "app/routers/self_update_router.py"}}
{"text": "elf_propose?objective=${encodeURIComponent(obj)}`);\n        const data = await res.json();\n        const patch = (data && typeof data.patch === \"string\") ? data.patch : \"\";\n        outEl.textContent = patch || data.detail || JSON.stringify(data, null, 2);\n\n        // 2) V√©rification c√¥t√© serveur (retourne syntax + verdict)\n        const vres = await fetch(\"/self_verify\", {\n          method: \"POST\",\n          headers: {\"Content-Type\":\"application/json\"},\n          body: JSON.stringify({ objective: obj, patch })\n        });\n        const vjson = await vres.json();\n        verEl.textContent = JSON.stringify(vjson, null, 2);\n\n        // 3) Journalisation (trace)\n        try {\n          const lres = await fetch(\"/log_code\", {\n            method: \"POST\",\n            headers: {\"Content-Type\":\"application/json\"},\n            body: JSON.stringify({ code: patch || \"(empty-patch)\", source: \"self_review\", meta: { objective: obj } })\n          });\n          const ljson = await lres.json();", "meta": {"source": "code_index", "path": "app/routers/self_update_router.py"}}
{"text": "{ code: patch || \"(empty-patch)\", source: \"self_review\", meta: { objective: obj } })\n          });\n          const ljson = await lres.json();\n          if (lres.ok && ljson && ljson.ok) {\n            statusEl.innerHTML = `<span class=\"ok\">‚úÖ Trace √©crite : ${ljson.path}</span>`;\n          } else {\n            statusEl.innerHTML = `<span class=\"err\">‚ùå Journalisation √©chou√©e</span>`;\n          }\n        } catch (e) {\n          console.warn(\"log_code failed:\", e);\n        }\n      } catch (e) {\n        outEl.textContent = \"Erreur: \" + e;\n        statusEl.innerHTML = `<span class=\"err\">‚ùå Exception: ${e}</span>`;\n        console.error(e);\n      }\n    }\n\n    async function applyPatch(){\n      const full = document.getElementById(\"out\").textContent;\n\n      // Tente d'extraire un bloc ```lang\\n...\\n```\n      const m = full.match(/```(\\w+)?\\\\s*\\\\n([\\\\s\\\\S]*?)```/m);\n      const code = m ? m[2] : full;\n\n      const file = prompt(\"Nom du fichier √† modifier (ex: app/services/search.py)\");\n      if (", "meta": {"source": "code_index", "path": "app/routers/self_update_router.py"}}
{"text": "\\\\s\\\\S]*?)```/m);\n      const code = m ? m[2] : full;\n\n      const file = prompt(\"Nom du fichier √† modifier (ex: app/services/search.py)\");\n      if (!file) return alert(\"Fichier non sp√©cifi√©\");\n\n      try {\n        const resp = await fetch(\"/apply_patch\", {\n          method: \"POST\",\n          headers: {\"Content-Type\":\"application/json\"},\n          body: JSON.stringify({ file_path: file, new_code: code })\n        });\n        const data = await resp.json();\n        if (!resp.ok) throw new Error(data.detail || \"Erreur HTTP\");\n        alert(\"‚úÖ \" + data.detail);\n        document.getElementById('status').textContent = \"üíæ Code appliqu√© avec succ√®s.\";\n      } catch (e) {\n        alert(\"‚ùå Erreur d‚Äôapplication : \" + e.message);\n        document.getElementById('status').textContent = \"‚ö†Ô∏è Erreur d'application du patch.\";\n      }\n    }\n  </script>\n</body>\n</html>\n    \"\"\".replace(\"__NOTES__\", items)\n\n    return html\n\n\n@router.get(\"/self_propose\")\ndef self_propose(objective: str = Query(..., min_len", "meta": {"source": "code_index", "path": "app/routers/self_update_router.py"}}
{"text": "body>\n</html>\n    \"\"\".replace(\"__NOTES__\", items)\n\n    return html\n\n\n@router.get(\"/self_propose\")\ndef self_propose(objective: str = Query(..., min_length=5)):\n    \"\"\"\n    G√©n√®re une proposition de patch (code brut) via le LLM local.\n    \"\"\"\n    try:\n        patch = propose_code_improvement(objective)\n        add_note(f\"Proposition g√©n√©r√©e pour: {objective}\")\n        return {\"ok\": True, \"patch\": patch}\n    except Exception as e:\n        return JSONResponse(status_code=500, content={\"ok\": False, \"detail\": str(e)})\n\n\n# ---- v√©rification serveur du patch g√©n√©r√© ----\n\nclass SelfVerifyIn(BaseModel):\n    objective: str = Field(..., min_length=3)\n    patch: str = Field(..., description=\"Code/patch propos√© (tel qu'affich√©).\")\n    context: Optional[str] = Field(default=None, description=\"Optionnel: extrait du code existant pour aider la v√©rification.\")\n\n@router.post(\"/self_verify\")\ndef self_verify(inp: SelfVerifyIn):\n    \"\"\"\n    V√©rifie automatiquement le patch d'Andy:\n      - check syntaxe loca", "meta": {"source": "code_index", "path": "app/routers/self_update_router.py"}}
{"text": "n.\")\n\n@router.post(\"/self_verify\")\ndef self_verify(inp: SelfVerifyIn):\n    \"\"\"\n    V√©rifie automatiquement le patch d'Andy:\n      - check syntaxe locale (si bloc ```lang ...)\n      - verdict structur√© par le LLM\n    \"\"\"\n    try:\n        result = verify_patch(inp.objective, inp.patch, context=inp.context)\n        return {\"ok\": True, **result}\n    except Exception as e:\n        return JSONResponse(status_code=500, content={\"ok\": False, \"detail\": str(e)})", "meta": {"source": "code_index", "path": "app/routers/self_update_router.py"}}
{"text": "# app/routers/trace.py\nfrom fastapi import APIRouter\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Dict, Any\nfrom app.services.trace_logger import write_code_trace, read_traces\n\nrouter = APIRouter(tags=[\"trace\"])\n\nclass TraceIn(BaseModel):\n    code: str = Field(..., description=\"Code g√©n√©r√© (texte brut)\")\n    source: str = Field(default=\"manual\", description=\"Origine (ex: self_review, chat, ui)\")\n    meta: Optional[Dict[str, Any]] = Field(default=None, description=\"Infos libres\")\n\n@router.post(\"/log_code\")\ndef log_code(inp: TraceIn):\n    # √©criture synchrone : garantit la cr√©ation du fichier imm√©diatement\n    result = write_code_trace(inp.code, inp.source, inp.meta)\n    return result\n\n@router.get(\"/logs\")\ndef logs(limit: int = 100):\n    return {\"items\": read_traces(limit)}", "meta": {"source": "code_index", "path": "app/routers/trace.py"}}
{"text": "Ôªøfrom fastapi import APIRouter\nimport requests\nfrom typing import Dict, Optional\n\nrouter = APIRouter()\nHEADERS = {\"User-Agent\": \"AssistantDylan/1.0 (+https://example.local)\"}\n\nWEATHER_DESC = {\n    0:\"ciel d√©gag√©\",1:\"peu nuageux\",2:\"partiellement nuageux\",3:\"couvert\",\n    45:\"brouillard\",48:\"brouillard givrant\",51:\"bruine faible\",53:\"bruine mod√©r√©e\",55:\"bruine forte\",\n    56:\"bruine vergla√ßante faible\",57:\"bruine vergla√ßante forte\",\n    61:\"pluie faible\",63:\"pluie mod√©r√©e\",65:\"pluie forte\",66:\"pluie vergla√ßante faible\",67:\"pluie vergla√ßante forte\",\n    71:\"neige faible\",73:\"neige mod√©r√©e\",75:\"neige forte\",77:\"neige en grains\",\n    80:\"averses faibles\",81:\"averses mod√©r√©es\",82:\"averses fortes\",\n    85:\"averses de neige faibles\",86:\"averses de neige fortes\",\n    95:\"orages\",96:\"orages avec gr√™le faible\",99:\"orages avec gr√™le forte\",\n}\ndef describe_weather(code: Optional[int]) -> str:\n    return WEATHER_DESC.get(code, f\"code m√©t√©o {code}\" if code is not None else \"conditions inconnues\")\n\nd", "meta": {"source": "code_index", "path": "app/routers/weather.py"}}
{"text": "scribe_weather(code: Optional[int]) -> str:\n    return WEATHER_DESC.get(code, f\"code m√©t√©o {code}\" if code is not None else \"conditions inconnues\")\n\ndef geocode_city(city: str) -> Dict:\n    url = \"https://geocoding-api.open-meteo.com/v1/search\"\n    params = {\"name\": city, \"count\": 1, \"language\": \"fr\", \"format\": \"json\"}\n    r = requests.get(url, params=params, headers=HEADERS, timeout=8); r.raise_for_status()\n    data = r.json(); results = data.get(\"results\") or []\n    if not results: raise ValueError(f\"Ville introuvable: {city}\")\n    top = results[0]\n    return {\"lat\": top[\"latitude\"], \"lon\": top[\"longitude\"], \"name\": top.get(\"name\"),\n            \"admin1\": top.get(\"admin1\"), \"country\": top.get(\"country\")}\n\ndef open_meteo_tomorrow(lat: float, lon: float) -> Dict:\n    url = (\"https://api.open-meteo.com/v1/forecast\"\n           f\"?latitude={lat}&longitude={lon}\"\n           \"&daily=temperature_2m_max,temperature_2m_min,precipitation_sum,weathercode\"\n           \"&timezone=Europe%2FParis\")", "meta": {"source": "code_index", "path": "app/routers/weather.py"}}
{"text": "at}&longitude={lon}\"\n           \"&daily=temperature_2m_max,temperature_2m_min,precipitation_sum,weathercode\"\n           \"&timezone=Europe%2FParis\")\n    r = requests.get(url, headers=HEADERS, timeout=8); r.raise_for_status()\n    return r.json()\n\n@router.get(\"/weather\")\ndef weather(city: str = \"Castres\"):\n    loc = geocode_city(city)\n    data = open_meteo_tomorrow(loc[\"lat\"], loc[\"lon\"])\n    daily = data.get(\"daily\", {})\n    def pick(i, key, default=None):\n        arr = daily.get(key) or []\n        return arr[i] if len(arr) > i else default\n    tmax = pick(1,\"temperature_2m_max\"); tmin = pick(1,\"temperature_2m_min\")\n    prcp = pick(1,\"precipitation_sum\"); code = pick(1,\"weathercode\")\n    return {\n        \"city\": f\"{loc['name']}, {loc['admin1']}, {loc['country']}\",\n        \"coords\": {\"lat\": loc[\"lat\"], \"lon\": loc[\"lon\"]},\n        \"tomorrow\": {\"tmin_c\": tmin, \"tmax_c\": tmax, \"precipitation_mm\": prcp,\n                     \"weathercode\": code, \"description\": describe_weather(code)}\n    }", "meta": {"source": "code_index", "path": "app/routers/weather.py"}}
{"text": "Ôªøfrom fastapi import FastAPI, Query, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Optional, Any\nimport requests, time, threading, json\nfrom urllib.parse import urlparse\nfrom urllib import robotparser\nfrom fastapi.responses import HTMLResponse, Response\n\nfrom duckduckgo_search import DDGS\nimport trafilatura\nfrom bs4 import BeautifulSoup\n\nprint(\">>> LOADED:\", __file__)\nUSER_NAME = \"Dylan\"\nHEADERS = {\"User-Agent\": \"AssistantDylan/1.0 (+https://example.local) Contact:dylan@example.local\"}\n\napp = FastAPI(title=\"Assistant de Dylan\")\n\n# ======================== Utils communs ========================\ndef extract_with_bs4(html: str, max_chars: int = 1600) -> str:\n    soup = BeautifulSoup(html, \"html.parser\")\n    for tag in soup([\"script\", \"style\", \"noscript\"]):\n        tag.decompose()\n    text = \" \".join(soup.get_text(separator=\" \").split())\n    return text[:max_chars]\n\ndef fetch_page_text(url: str, timeout: int = 8, max_chars: int = 1400) -> str:\n    # 1) trafila", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "t(separator=\" \").split())\n    return text[:max_chars]\n\ndef fetch_page_text(url: str, timeout: int = 8, max_chars: int = 1400) -> str:\n    # 1) trafilatura.fetch_url -> extract\n    try:\n        downloaded = trafilatura.fetch_url(url, timeout=timeout)\n        if downloaded:\n            txt = trafilatura.extract(downloaded) or \"\"\n            if txt.strip():\n                return txt.strip()[:max_chars]\n    except Exception:\n        pass\n    # 2) requests + extract + fallback BS4\n    try:\n        resp = requests.get(url, headers=HEADERS, timeout=timeout)\n        if resp.ok and resp.text:\n            txt = trafilatura.extract(resp.text) or \"\"\n            if txt.strip():\n                return txt.strip()[:max_chars]\n            return extract_with_bs4(resp.text, max_chars=max_chars)\n    except Exception:\n        pass\n    return \"\"\n\n# ======================== Routes de base ========================\nclass ChatIn(BaseModel):\n    message: str\n\nclass ChatOut(BaseModel):\n    reply: str\n\n@app.get", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "================ Routes de base ========================\nclass ChatIn(BaseModel):\n    message: str\n\nclass ChatOut(BaseModel):\n    reply: str\n\n@app.get(\"/\")\ndef root():\n    return {\"ok\": True, \"msg\": f\"Bonjour {USER_NAME}, l‚ÄôAPI est pr√™te.\"}\n\n@app.get(\"/routes\")\ndef routes():\n    return [r.path for r in app.router.routes]\n\n@app.post(\"/chat\", response_model=ChatOut)\ndef chat(inp: ChatIn):\n    return ChatOut(reply=f\"Bonjour {USER_NAME} üëã Tu as dit : ‚Äú{inp.message}‚Äù.\")\n\n# ======================== Recherche simple ========================\n@app.get(\"/web_fast\")\ndef web_fast(q: str = \"tendances IA en entreprise 2025\", max_results: int = 3):\n    out = []\n    try:\n        with DDGS() as ddgs:\n            for r in ddgs.text(q, region=\"fr-fr\", safesearch=\"moderate\", max_results=max_results):\n                url = r.get(\"href\") or r.get(\"url\")\n                if url:\n                    out.append({\"title\": r.get(\"title\") or \"R√©sultat\", \"url\": url})\n    except Exception as e:\n        return {\"quer", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "if url:\n                    out.append({\"title\": r.get(\"title\") or \"R√©sultat\", \"url\": url})\n    except Exception as e:\n        return {\"query\": q, \"error\": str(e), \"results\": out}\n    return {\"query\": q, \"results\": out}\n\n@app.get(\"/web_test\")\ndef web_test(q: str = \"site:wikipedia.org intelligence artificielle\", max_results: int = 2):\n    results = []\n    try:\n        with DDGS() as ddgs:\n            for r in ddgs.text(q, region=\"fr-fr\", safesearch=\"moderate\", max_results=max_results):\n                url = r.get(\"href\") or r.get(\"url\")\n                if not url:\n                    continue\n                extract = fetch_page_text(url, timeout=8, max_chars=1400)\n                results.append({\n                    \"title\": r.get(\"title\") or \"R√©sultat\",\n                    \"url\": url,\n                    \"snippet\": (r.get(\"body\") or \"\")[:220],\n                    \"extract\": extract\n                })\n                time.sleep(0.4)\n    except Exception as e:\n        results.", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "\") or \"\")[:220],\n                    \"extract\": extract\n                })\n                time.sleep(0.4)\n    except Exception as e:\n        results.append({\"title\": \"ERREUR\", \"url\": \"\", \"snippet\": str(e), \"extract\": \"\"})\n    return {\"query\": q, \"results\": results}\n\n# ======================== M√©t√©o (Open-Meteo) ========================\nWEATHER_DESC = {\n    0:\"ciel d√©gag√©\",1:\"peu nuageux\",2:\"partiellement nuageux\",3:\"couvert\",\n    45:\"brouillard\",48:\"brouillard givrant\",51:\"bruine faible\",53:\"bruine mod√©r√©e\",55:\"bruine forte\",\n    56:\"bruine vergla√ßante faible\",57:\"bruine vergla√ßante forte\",\n    61:\"pluie faible\",63:\"pluie mod√©r√©e\",65:\"pluie forte\",66:\"pluie vergla√ßante faible\",67:\"pluie vergla√ßante forte\",\n    71:\"neige faible\",73:\"neige mod√©r√©e\",75:\"neige forte\",77:\"neige en grains\",\n    80:\"averses faibles\",81:\"averses mod√©r√©es\",82:\"averses fortes\",\n    85:\"averses de neige faibles\",86:\"averses de neige fortes\",\n    95:\"orages\",96:\"orages avec gr√™le faible\",99:\"orages avec gr√™le for", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "erses fortes\",\n    85:\"averses de neige faibles\",86:\"averses de neige fortes\",\n    95:\"orages\",96:\"orages avec gr√™le faible\",99:\"orages avec gr√™le forte\",\n}\ndef describe_weather(code: Optional[int]) -> str:\n    return WEATHER_DESC.get(code, f\"code m√©t√©o {code}\" if code is not None else \"conditions inconnues\")\n\ndef geocode_city(city: str) -> Dict:\n    url = \"https://geocoding-api.open-meteo.com/v1/search\"\n    params = {\"name\": city, \"count\": 1, \"language\": \"fr\", \"format\": \"json\"}\n    r = requests.get(url, params=params, headers=HEADERS, timeout=8); r.raise_for_status()\n    data = r.json(); results = data.get(\"results\") or []\n    if not results:\n        raise ValueError(f\"Ville introuvable: {city}\")\n    top = results[0]\n    return {\"lat\": top[\"latitude\"], \"lon\": top[\"longitude\"], \"name\": top.get(\"name\"),\n            \"admin1\": top.get(\"admin1\"), \"country\": top.get(\"country\")}\n\ndef open_meteo_tomorrow(lat: float, lon: float) -> Dict:\n    url = (\"https://api.open-meteo.com/v1/forecast\"", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "1\"), \"country\": top.get(\"country\")}\n\ndef open_meteo_tomorrow(lat: float, lon: float) -> Dict:\n    url = (\"https://api.open-meteo.com/v1/forecast\"\n           f\"?latitude={lat}&longitude={lon}\"\n           \"&daily=temperature_2m_max,temperature_2m_min,precipitation_sum,weathercode\"\n           \"&timezone=Europe%2FParis\")\n    r = requests.get(url, headers=HEADERS, timeout=8); r.raise_for_status()\n    return r.json()\n\n@app.get(\"/weather\")\ndef weather(city: str = \"Castres\"):\n    loc = geocode_city(city)\n    data = open_meteo_tomorrow(loc[\"lat\"], loc[\"lon\"])\n    daily = data.get(\"daily\", {})\n    def pick(i, key, default=None):\n        arr = daily.get(key) or []\n        return arr[i] if len(arr) > i else default\n    tmax = pick(1, \"temperature_2m_max\")\n    tmin = pick(1, \"temperature_2m_min\")\n    prcp = pick(1, \"precipitation_sum\")\n    code = pick(1, \"weathercode\")\n    return {\n        \"city\": f\"{loc['name']}, {loc['admin1']}, {loc['country']}\",\n        \"coords\": {\"lat\": loc[\"lat\"], \"lon\": loc[", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "(1, \"weathercode\")\n    return {\n        \"city\": f\"{loc['name']}, {loc['admin1']}, {loc['country']}\",\n        \"coords\": {\"lat\": loc[\"lat\"], \"lon\": loc[\"lon\"]},\n        \"tomorrow\": {\"tmin_c\": tmin, \"tmax_c\": tmax, \"precipitation_mm\": prcp,\n                     \"weathercode\": code, \"description\": describe_weather(code)}\n    }\n\n# ======================== Deep Search (pouss√© & responsable) ========================\nDEFAULT_MAX_RESULTS = 50\nDEFAULT_PER_DOMAIN = 5\nDEFAULT_DELAY_PER_DOMAIN = 1.0\nREQUEST_TIMEOUT = 8\n\n_robots_cache: Dict[str, Optional[robotparser.RobotFileParser]] = {}\n_last_access: Dict[str, float] = {}\n_count_per_domain: Dict[str, int] = {}\n_lock = threading.Lock()\n\nclass SearchResult(BaseModel):\n    title: Optional[str]\n    url: str\n    snippet: Optional[str]\n    extract: Optional[str] = None\n    allowed_by_robots: Optional[bool] = None\n    domain: Optional[str] = None\n\nclass DeepSearchOut(BaseModel):\n    query: str\n    results: List[SearchResult]\n    meta: Dict[str, Any]\n\ndef", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "= None\n    domain: Optional[str] = None\n\nclass DeepSearchOut(BaseModel):\n    query: str\n    results: List[SearchResult]\n    meta: Dict[str, Any]\n\ndef get_domain(url: str) -> str:\n    try:\n        return urlparse(url).netloc.lower()\n    except Exception:\n        return \"\"\n\ndef can_fetch_url(url: str, user_agent: str = HEADERS[\"User-Agent\"]) -> bool:\n    domain = get_domain(url)\n    if not domain:\n        return False\n    base = f\"{urlparse(url).scheme}://{domain}\"\n    rp = _robots_cache.get(base)\n    if rp is None:\n        rp = robotparser.RobotFileParser()\n        try:\n            rp.set_url(base + \"/robots.txt\")\n            rp.read()\n            _robots_cache[base] = rp\n        except Exception:\n            _robots_cache[base] = None\n            return True\n    if rp is None:\n        return True\n    return rp.can_fetch(user_agent, url)\n\ndef polite_wait(domain: str, min_interval: float = DEFAULT_DELAY_PER_DOMAIN):\n    with _lock:\n        last = _last_access.get(domain)\n        now = t", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "def polite_wait(domain: str, min_interval: float = DEFAULT_DELAY_PER_DOMAIN):\n    with _lock:\n        last = _last_access.get(domain)\n        now = time.time()\n        if last:\n            wait = min_interval - (now - last)\n            if wait > 0:\n                time.sleep(wait)\n        _last_access[domain] = time.time()\n\ndef increment_domain_count(domain: str) -> int:\n    with _lock:\n        c = _count_per_domain.get(domain, 0) + 1\n        _count_per_domain[domain] = c\n        return c\n\ndef fetch_metadata(url: str) -> Dict[str, Optional[str]]:\n    resp = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n    resp.raise_for_status()\n    html = resp.text\n    # trafilatura extract\n    try:\n        txt = trafilatura.extract(html) or \"\"\n        extract = \" \".join(txt.split())[:500] if txt and len(txt.strip()) > 40 else \"\"\n    except Exception:\n        extract = \"\"\n    # title + meta description\n    soup = BeautifulSoup(html, \"html.parser\")\n    title_tag = soup.find(\"title\")", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "pt Exception:\n        extract = \"\"\n    # title + meta description\n    soup = BeautifulSoup(html, \"html.parser\")\n    title_tag = soup.find(\"title\")\n    title = title_tag.get_text(strip=True) if title_tag else None\n    meta = soup.find(\"meta\", attrs={\"name\": \"description\"}) or soup.find(\"meta\", attrs={\"property\": \"og:description\"})\n    snippet = meta.get(\"content\").strip() if meta and meta.get(\"content\") else None\n    if not snippet and not extract:\n        node = soup.find(\"p\")\n        if node and node.get_text(strip=True):\n            snippet = node.get_text(strip=True)[:300]\n    return {\"title\": title, \"snippet\": snippet, \"extract\": extract}\n\ndef ddg_search_raw(query: str, max_results: int = 50) -> List[Dict]:\n    results = []\n    with DDGS() as ddgs:\n        for r in ddgs.text(query, region=\"fr-fr\", safesearch=\"moderate\", max_results=max_results):\n            results.append(r)\n    return results\n\n@app.get(\"/deep_search\", response_model=DeepSearchOut)\ndef deep_search(\n    q: str = Que", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "x_results):\n            results.append(r)\n    return results\n\n@app.get(\"/deep_search\", response_model=DeepSearchOut)\ndef deep_search(\n    q: str = Query(..., min_length=2),\n    max_results: int = Query(DEFAULT_MAX_RESULTS, ge=1, le=200),\n    follow: bool = Query(False),\n    max_per_domain: int = Query(DEFAULT_PER_DOMAIN, ge=1, le=50),\n    delay_per_domain: float = Query(DEFAULT_DELAY_PER_DOMAIN, ge=0.0, le=10.0),\n    pretty: bool = Query(False)\n):\n    if not q or len(q.strip()) < 2:\n        raise HTTPException(status_code=400, detail=\"Query trop courte\")\n    max_results = min(max_results, 200)\n\n    try:\n        raw = ddg_search_raw(q, max_results=max_results)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Erreur moteur: {e}\")\n\n    results_out: List[SearchResult] = []\n    total_followed = 0\n    domain_counts_local: Dict[str, int] = {}\n\n    for r in raw:\n        url = r.get(\"href\") or r.get(\"url\")\n        title = r.get(\"title\") or None\n        snippet_fr", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "ts_local: Dict[str, int] = {}\n\n    for r in raw:\n        url = r.get(\"href\") or r.get(\"url\")\n        title = r.get(\"title\") or None\n        snippet_from_search = (r.get(\"body\") or \"\")[:300] or None\n        if not url:\n            continue\n\n        domain = get_domain(url)\n        allowed = can_fetch_url(url)\n\n        item = SearchResult(\n            title=title, url=url, snippet=snippet_from_search,\n            extract=None, allowed_by_robots=allowed, domain=domain\n        )\n\n        if follow and allowed:\n            if domain_counts_local.get(domain, 0) < max_per_domain:\n                polite_wait(domain, min_interval=delay_per_domain)\n                c = increment_domain_count(domain)\n                if c <= max_per_domain:\n                    try:\n                        meta = fetch_metadata(url)\n                        item.title = item.title or meta.get(\"title\")\n                        item.snippet = item.snippet or meta.get(\"snippet\")\n                        item.extract = met", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "item.title or meta.get(\"title\")\n                        item.snippet = item.snippet or meta.get(\"snippet\")\n                        item.extract = meta.get(\"extract\") or None\n                        total_followed += 1\n                    except Exception as e:\n                        item.extract = f\"ERROR_FETCH:{str(e)}\"\n                domain_counts_local[domain] = domain_counts_local.get(domain, 0) + 1\n\n        results_out.append(item)\n        if len(results_out) >= max_results:\n            break\n\n    meta = {\n        \"total_results_from_search\": len(raw),\n        \"returned_results\": len(results_out),\n        \"follow_performed\": follow,\n        \"total_followed\": total_followed,\n        \"max_per_domain\": max_per_domain,\n        \"delay_per_domain\": delay_per_domain,\n    }\n\n    result = {\"query\": q, \"results\": results_out, \"meta\": meta}\n\n    if pretty:\n        return Response(\n            content=json.dumps(result, ensure_ascii=False, indent=2),\n            media_type=\"application/jso", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "if pretty:\n        return Response(\n            content=json.dumps(result, ensure_ascii=False, indent=2),\n            media_type=\"application/json\",\n        )\n    return result\n\n# ======================== UI (barre de recherche) ========================\n@app.get(\"/ui\", response_class=HTMLResponse)\ndef ui():\n    return \"\"\"\n<!doctype html>\n<html lang=\"fr\">\n<head>\n  <meta charset=\"utf-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n  <title>Assistant de Dylan ‚Äî Deep Search</title>\n  <script src=\"https://cdn.tailwindcss.com\"></script>\n  <style>\n    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, \"Liberation Mono\", monospace; }\n    .card { border-radius: 1rem; box-shadow: 0 6px 24px rgba(0,0,0,.08); }\n  </style>\n</head>\n<body class=\"bg-gray-50 text-gray-900\">\n  <div class=\"max-w-5xl mx-auto p-6\">\n    <h1 class=\"text-2xl font-semibold mb-4\">üîé Assistant de Dylan ‚Äî Deep Search</h1>\n\n    <div class=\"card bg-white p-4 mb-6\">\n      <", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "xl mx-auto p-6\">\n    <h1 class=\"text-2xl font-semibold mb-4\">üîé Assistant de Dylan ‚Äî Deep Search</h1>\n\n    <div class=\"card bg-white p-4 mb-6\">\n      <div class=\"grid grid-cols-1 md:grid-cols-6 gap-3 items-end\">\n        <div class=\"md:col-span-3\">\n          <label class=\"block text-sm mb-1\">Requ√™te</label>\n          <input id=\"q\" type=\"text\" placeholder=\"ex: tendances IA en entreprise 2025\"\n                 class=\"w-full rounded-lg border px-3 py-2 focus:outline-none focus:ring focus:ring-indigo-200\" />\n        </div>\n        <div>\n          <label class=\"block text-sm mb-1\">Max r√©sultats</label>\n          <input id=\"max_results\" type=\"number\" min=\"1\" max=\"200\" value=\"30\"\n                 class=\"w-full rounded-lg border px-3 py-2 focus:outline-none focus:ring focus:ring-indigo-200\" />\n        </div>\n        <div>\n          <label class=\"block text-sm mb-1\">Max/domain</label>\n          <input id=\"max_per_domain\" type=\"number\" min=\"1\" max=\"50\" value=\"3\"\n                 class=\"w-full roun", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "ock text-sm mb-1\">Max/domain</label>\n          <input id=\"max_per_domain\" type=\"number\" min=\"1\" max=\"50\" value=\"3\"\n                 class=\"w-full rounded-lg border px-3 py-2 focus:outline-none focus:ring focus:ring-indigo-200\" />\n        </div>\n        <div>\n          <label class=\"block text-sm mb-1\">Delay/domain (s)</label>\n          <input id=\"delay_per_domain\" type=\"number\" step=\"0.1\" min=\"0\" max=\"10\" value=\"1.2\"\n                 class=\"w-full rounded-lg border px-3 py-2 focus:outline-none focus:ring focus:ring-indigo-200\" />\n        </div>\n        <div class=\"flex items-center gap-3\">\n          <label class=\"inline-flex items-center gap-2\">\n            <input id=\"follow\" type=\"checkbox\" class=\"h-4 w-4\" />\n            <span>Suivre liens</span>\n          </label>\n          <label class=\"inline-flex items-center gap-2\">\n            <input id=\"pretty\" type=\"checkbox\" class=\"h-4 w-4\" />\n            <span>JSON joli</span>\n          </label>\n        </div>\n        <div class=\"md:col-span", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "ut id=\"pretty\" type=\"checkbox\" class=\"h-4 w-4\" />\n            <span>JSON joli</span>\n          </label>\n        </div>\n        <div class=\"md:col-span-6 flex gap-3\">\n          <button id=\"go\" class=\"rounded-lg bg-indigo-600 text-white px-4 py-2 hover:bg-indigo-700\">\n            Rechercher\n          </button>\n          <button id=\"clear\" class=\"rounded-lg bg-gray-200 text-gray-800 px-4 py-2 hover:bg-gray-300\">\n            Effacer\n          </button>\n        </div>\n      </div>\n    </div>\n\n    <div id=\"status\" class=\"text-sm text-gray-600 mb-3\"></div>\n    <div id=\"results\" class=\"space-y-4\"></div>\n\n    <details class=\"mt-8\">\n      <summary class=\"cursor-pointer text-sm text-gray-600\">Voir la r√©ponse JSON brute</summary>\n      <pre id=\"raw\" class=\"mono text-xs bg-white p-4 rounded-lg overflow-auto mt-2\"></pre>\n    </details>\n  </div>\n\n<script>\nconst $ = (sel) => document.querySelector(sel);\nconst esc = (s) => (s || \"\").toString().replace(/[&<>]/g, c => ({'&':'&amp;','<':'&lt;','>':'&gt;'}", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "t>\nconst $ = (sel) => document.querySelector(sel);\nconst esc = (s) => (s || \"\").toString().replace(/[&<>]/g, c => ({'&':'&amp;','<':'&lt;','>':'&gt;'}[c]) );\n\nasync function run() {\n  const q = $(\"#q\").value.trim();\n  if (!q) { $(\"#status\").textContent = \"Saisis une requ√™te.\"; return; }\n\n  const max_results = +$(\"#max_results\").value || 30;\n  const follow = $(\"#follow\").checked;\n  const max_per_domain = +$(\"#max_per_domain\").value || 3;\n  const delay_per_domain = +$(\"#delay_per_domain\").value || 1.2;\n  const pretty = $(\"#pretty\").checked;\n\n  const params = new URLSearchParams({\n    q, max_results, follow, max_per_domain, delay_per_domain, pretty\n  });\n\n  const url = `/deep_search?${params.toString()}`;\n  $(\"#status\").textContent = \"Recherche en cours‚Ä¶\";\n  $(\"#results\").innerHTML = \"\";\n  $(\"#raw\").textContent = \"\";\n\n  try {\n    const res = await fetch(url);\n    const txt = await res.text();\n\n    let data;\n    try { data = JSON.parse(txt); } catch { data = null; }\n\n    if (data && data.r", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "await fetch(url);\n    const txt = await res.text();\n\n    let data;\n    try { data = JSON.parse(txt); } catch { data = null; }\n\n    if (data && data.results) {\n      renderCards(data);\n      $(\"#raw\").textContent = JSON.stringify(data, null, 2);\n      $(\"#status\").textContent = `OK ‚Äî ${data.results.length} r√©sultats (follow=${data.meta.follow_performed ? \"oui\" : \"non\"})`;\n    } else {\n      $(\"#raw\").textContent = txt;\n      $(\"#status\").textContent = \"R√©ponse format√©e (pretty)\";\n    }\n  } catch (e) {\n    $(\"#status\").textContent = \"Erreur: \" + e.message;\n  }\n}\n\nfunction renderCards(data) {\n  const wrap = $(\"#results\");\n  wrap.innerHTML = \"\";\n  for (const it of data.results) {\n    const allowed = it.allowed_by_robots === false\n      ? '<span class=\"text-xs bg-red-100 text-red-700 px-2 py-1 rounded-full\">robots.txt: non</span>'\n      : '<span class=\"text-xs bg-green-100 text-green-700 px-2 py-1 rounded-full\">robots.txt: ok</span>';\n\n    const card = document.createElement(\"div\");\n    ca", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "pan class=\"text-xs bg-green-100 text-green-700 px-2 py-1 rounded-full\">robots.txt: ok</span>';\n\n    const card = document.createElement(\"div\");\n    card.className = \"card bg-white p-4\";\n    card.innerHTML = `\n      <div class=\"flex flex-wrap items-center gap-2 mb-2\">\n        <a class=\"text-lg font-medium text-indigo-700 hover:underline\" href=\"${esc(it.url)}\" target=\"_blank\" rel=\"noopener\">\n          ${esc(it.title || it.url)}\n        </a>\n        <span class=\"text-xs bg-gray-100 text-gray-700 px-2 py-1 rounded-full\">${esc(it.domain || \"\")}</span>\n        ${allowed}\n      </div>\n      ${it.snippet ? `<p class=\"text-sm text-gray-700 mb-2\">${esc(it.snippet)}</p>` : ``}\n      ${it.extract ? `<details class=\"mt-1\">\n          <summary class=\"text-sm text-gray-600 cursor-pointer\">Extrait</summary>\n          <p class=\"text-sm text-gray-800 mt-1\">${esc(it.extract)}</p>\n      </details>` : ``}\n      <div class=\"mt-2 text-xs text-gray-500 mono\">${esc(it.url)}</div>\n    `;\n    wrap.appendChild(car", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "${esc(it.extract)}</p>\n      </details>` : ``}\n      <div class=\"mt-2 text-xs text-gray-500 mono\">${esc(it.url)}</div>\n    `;\n    wrap.appendChild(card);\n  }\n}\n\ndocument.addEventListener(\"DOMContentLoaded\", () => {\n  $(\"#go\").addEventListener(\"click\", run);\n  $(\"#clear\").addEventListener(\"click\", () => {\n    $(\"#q\").value = \"\";\n    $(\"#results\").innerHTML = \"\";\n    $(\"#raw\").textContent = \"\";\n    $(\"#status\").textContent = \"\";\n  });\n  $(\"#q\").addEventListener(\"keydown\", (e) => { if (e.key === \"Enter\") run(); });\n});\n</script>\n</body>\n</html>\n    \"\"\"", "meta": {"source": "code_index", "path": "app/server.py"}}
{"text": "Ôªøfrom fastapi import FastAPI\nprint('>>> LOADED (min):', __file__)\napp = FastAPI(title='Smoke Test')\n@app.get('/')\ndef root():\n    return {'ok': True, 'msg': 'root ok'}\n@app.get('/routes')\ndef routes():\n    return [r.path for r in app.router.routes]", "meta": {"source": "code_index", "path": "app/server_min.py"}}
{"text": "\"\"\"\nserver_search.py\nEndpoint responsable pour recherche \"pouss√©e\" :\n- /deep_search?q=...&max_results=50&follow=true\n- Respecte robots.txt, rate-limit par domaine, plafond par domaine, d√©lai global.\n\"\"\"\n\nfrom fastapi import FastAPI, Query, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional, Dict, Any\nfrom duckduckgo_search import DDGS\nfrom urllib.parse import urlparse\nfrom urllib import robotparser\nimport requests\nimport time\nimport trafilatura\nfrom bs4 import BeautifulSoup\nimport threading\n\n# ---------- Configurable ----------\nUSER_AGENT = \"AssistantDylan/1.0 (+https://example.local contact:dylan@example.local)\"\nDEFAULT_MAX_RESULTS = 50           # plafond global (ne pas trop pousser)\nDEFAULT_PER_DOMAIN = 5             # max pages suivies par domaine\nDEFAULT_DELAY_PER_DOMAIN = 1.0     # secondes entre requ√™tes sur le m√™me domaine\nREQUEST_TIMEOUT = 8                # timeout HTTP\n# ----------------------------------\n\napp = FastAPI(title=\"Assistant Search (po", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "sur le m√™me domaine\nREQUEST_TIMEOUT = 8                # timeout HTTP\n# ----------------------------------\n\napp = FastAPI(title=\"Assistant Search (pouss√© et responsable)\")\n\n# In-memory caches / counters (process-local)\n_robots_cache: Dict[str, Optional[robotparser.RobotFileParser]] = {}\n_last_access: Dict[str, float] = {}\n_count_per_domain: Dict[str, int] = {}\n_lock = threading.Lock()  # prot√®ge _last_access et _count_per_domain\n\n# ---------- Models ----------\nclass SearchResult(BaseModel):\n    title: Optional[str]\n    url: str\n    snippet: Optional[str]\n    extract: Optional[str] = None  # petit extrait si follow True\n    allowed_by_robots: Optional[bool] = None\n    domain: Optional[str] = None\n\nclass DeepSearchOut(BaseModel):\n    query: str\n    results: List[SearchResult]\n    meta: Dict[str, Any]\n\n# ---------- Polite helpers ----------\ndef get_domain(url: str) -> str:\n    try:\n        p = urlparse(url)\n        return p.netloc.lower()\n    except Exception:\n        return \"\"\n\ndef can_", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "def get_domain(url: str) -> str:\n    try:\n        p = urlparse(url)\n        return p.netloc.lower()\n    except Exception:\n        return \"\"\n\ndef can_fetch_url(url: str, user_agent: str = USER_AGENT) -> bool:\n    domain = get_domain(url)\n    if not domain:\n        return False\n    base = f\"{urlparse(url).scheme}://{domain}\"\n    rp = _robots_cache.get(base)\n    if rp is None:\n        rp = robotparser.RobotFileParser()\n        try:\n            rp.set_url(base + \"/robots.txt\")\n            rp.read()\n            _robots_cache[base] = rp\n        except Exception:\n            # si robots.txt non disponible, on choisit la posture prudente : permettre mais avec limites appliqu√©es\n            _robots_cache[base] = None\n            return True\n    if rp is None:\n        return True\n    return rp.can_fetch(user_agent, url)\n\ndef polite_wait(domain: str, min_interval: float = DEFAULT_DELAY_PER_DOMAIN):\n    with _lock:\n        last = _last_access.get(domain)\n        now = time.time()\n        if last:", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "r, min_interval: float = DEFAULT_DELAY_PER_DOMAIN):\n    with _lock:\n        last = _last_access.get(domain)\n        now = time.time()\n        if last:\n            wait = min_interval - (now - last)\n            if wait > 0:\n                time.sleep(wait)\n        _last_access[domain] = time.time()\n\ndef increment_domain_count(domain: str) -> int:\n    with _lock:\n        c = _count_per_domain.get(domain, 0) + 1\n        _count_per_domain[domain] = c\n        return c\n\n# ---------- Fetch minimal metadata (title + meta description or first text snippet) ----------\ndef fetch_metadata(url: str) -> Dict[str, Optional[str]]:\n    \"\"\"\n    R√©cup√®re en mode poli : title, meta description, extrait court.\n    Respecte timeout et user-agent. N'essaie pas d'extraire tout le site.\n    \"\"\"\n    headers = {\"User-Agent\": USER_AGENT}\n    resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n    resp.raise_for_status()\n    html = resp.text\n\n    # 1) essayer trafilatura (souvent meilleur pour conte", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "headers=headers, timeout=REQUEST_TIMEOUT)\n    resp.raise_for_status()\n    html = resp.text\n\n    # 1) essayer trafilatura (souvent meilleur pour contenu principal)\n    try:\n        txt = trafilatura.extract(html) or \"\"\n        if txt and len(txt.strip()) > 40:\n            # retourner un d√©but lisible\n            extract = \" \".join(txt.strip().split())[:500]\n        else:\n            extract = \"\"\n    except Exception:\n        extract = \"\"\n\n    # 2) r√©cup√©rer title + meta description via BeautifulSoup (rapide)\n    soup = BeautifulSoup(html, \"html.parser\")\n    title_tag = soup.find(\"title\")\n    title = title_tag.get_text(strip=True) if title_tag else None\n    meta = soup.find(\"meta\", attrs={\"name\":\"description\"}) or soup.find(\"meta\", attrs={\"property\":\"og:description\"})\n    snippet = meta.get(\"content\").strip() if meta and meta.get(\"content\") else None\n\n    # fallback snippet to first text excerpt (small)\n    if not snippet and not extract:\n        # take some visible text\n        for sel", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": ") else None\n\n    # fallback snippet to first text excerpt (small)\n    if not snippet and not extract:\n        # take some visible text\n        for sel in [\"p\", \"h1\", \"h2\", \"article\"]:\n            node = soup.find(sel)\n            if node and node.get_text(strip=True):\n                snippet = node.get_text(strip=True)[:300]\n                break\n\n    return {\"title\": title, \"snippet\": snippet, \"extract\": extract}\n\n# ---------- Core: perform search (links from DDG) ----------\ndef ddg_search_raw(query: str, max_results: int = 50) -> List[Dict]:\n    \"\"\"Retourne la liste brute dicts depuis duckduckgo_search (title, href, body...).\"\"\"\n    results = []\n    with DDGS() as ddgs:\n        for r in ddgs.text(query, region=\"fr-fr\", safesearch=\"moderate\", max_results=max_results):\n            results.append(r)\n    return results\n\n# ---------- Endpoint principal ----------\n@app.get(\"/deep_search\", response_model=DeepSearchOut)\ndef deep_search(\n    q: str = Query(..., min_length=2, description=\"Requ", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "t principal ----------\n@app.get(\"/deep_search\", response_model=DeepSearchOut)\ndef deep_search(\n    q: str = Query(..., min_length=2, description=\"Requ√™te de recherche\"),\n    max_results: int = Query(DEFAULT_MAX_RESULTS, ge=1, le=200, description=\"Nombre maximum de r√©sultats (global)\"),\n    follow: bool = Query(False, description=\"Si true, on suit chaque lien (poliment) et r√©cup√®re titre/snippet/extract\"),\n    max_per_domain: int = Query(DEFAULT_PER_DOMAIN, ge=1, le=50, description=\"Max pages suivies par domaine\"),\n    delay_per_domain: float = Query(DEFAULT_DELAY_PER_DOMAIN, ge=0.0, le=10.0, description=\"D√©lai minimum (s) entre requ√™tes sur un m√™me domaine\")\n):\n    # v√©rifications basiques\n    if not q or len(q.strip()) < 2:\n        raise HTTPException(status_code=400, detail=\"Query trop courte\")\n    max_results = min(max_results, 200)\n\n    # Step 1: obtenir les r√©sultats du moteur (liens)\n    try:\n        raw = ddg_search_raw(q, max_results=max_results)\n    except Exception as e:", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "# Step 1: obtenir les r√©sultats du moteur (liens)\n    try:\n        raw = ddg_search_raw(q, max_results=max_results)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Erreur moteur: {e}\")\n\n    results_out: List[SearchResult] = []\n    total_followed = 0\n    domain_counts_local: Dict[str, int] = {}\n\n    # Step 2: parcourir les r√©sultats, optionnellement suivre\n    for r in raw:\n        url = r.get(\"href\") or r.get(\"url\")\n        title = r.get(\"title\") or None\n        snippet_from_search = (r.get(\"body\") or \"\")[:300] or None\n        if not url:\n            continue\n\n        domain = get_domain(url)\n        allowed = can_fetch_url(url)\n\n        item = SearchResult(\n            title=title,\n            url=url,\n            snippet=snippet_from_search,\n            extract=None,\n            allowed_by_robots=allowed,\n            domain=domain\n        )\n\n        # if follow requested and allowed and we haven't exceeded per-domain caps:\n        if follow and all", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "wed,\n            domain=domain\n        )\n\n        # if follow requested and allowed and we haven't exceeded per-domain caps:\n        if follow and allowed:\n            # per-domain counting\n            domain_count = domain_counts_local.get(domain, 0)\n            if domain_count >= max_per_domain:\n                # skip following this domain further\n                item.extract = None\n            else:\n                # wait politly\n                polite_wait(domain, min_interval=delay_per_domain)\n                # re-check global/process-level per-domain count\n                c = increment_domain_count(domain)\n                if c > max_per_domain:\n                    # exceeded global process limit for this domain\n                    item.extract = None\n                else:\n                    # attempt fetch metadata, protected by try/except\n                    try:\n                        meta = fetch_metadata(url)\n                        item.title = item.title or meta.get(\"titl", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "y/except\n                    try:\n                        meta = fetch_metadata(url)\n                        item.title = item.title or meta.get(\"title\")\n                        item.snippet = item.snippet or meta.get(\"snippet\")\n                        # prefer trafilatura extract if available\n                        item.extract = meta.get(\"extract\") or None\n                        total_followed += 1\n                    except Exception as e:\n                        # don't stop the whole pipeline; log in extract\n                        item.extract = f\"ERROR_FETCH:{str(e)}\"\n                domain_counts_local[domain] = domain_counts_local.get(domain, 0) + 1\n\n        results_out.append(item)\n\n        # stop early if we've already processed enough raw results (respect max_results)\n        if len(results_out) >= max_results:\n            break\n\n    meta = {\n        \"total_results_from_search\": len(raw),\n        \"returned_results\": len(results_out),\n        \"follow_performed\": follow,", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "reak\n\n    meta = {\n        \"total_results_from_search\": len(raw),\n        \"returned_results\": len(results_out),\n        \"follow_performed\": follow,\n        \"total_followed\": total_followed,\n        \"max_per_domain\": max_per_domain,\n        \"delay_per_domain\": delay_per_domain\n    }\n\n    return DeepSearchOut(query=q, results=results_out, meta=meta)", "meta": {"source": "code_index", "path": "app/server_search.py"}}
{"text": "Ôªø", "meta": {"source": "code_index", "path": "app/services/__init__.py"}}
{"text": "# app/services/chat_engine.py\nimport json\nimport os\nimport requests\nfrom app.services.memory import add_to_memory, search_memory\nfrom app.services.search import hybrid_search, follow_and_build\nfrom app.services.memory import search_memory\n# AVANT\n# from app.services.memory import add_to_memory, search_memory\n\n# APR√àS\nfrom app.services.memory import save_text_memory, search_memory\n\n\nSYSTEM_IDENTITY = \"\"\"\nTu es Andy, une IA ex√©cut√©e localement et cr√©√©e par Dylan.\nTu ob√©is √† Dylan, tu es utile, claire et logique.\nTu a un acc√®s direct √† Internet, tu utilises les fonctions de recherche fournies.\nApprends continuellement √† partir des √©changes (m√©moire locale).\n\"\"\"\n\n# üîß Lis la config depuis l'environnement\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"127.0.0.1\")\nOLLAMA_PORT = int(os.getenv(\"OLLAMA_PORT\", \"11434\"))\nOLLAMA_URL = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/generate\"\nMODEL_NAME = os.getenv(\"OLLAMA_MODEL\", \"phi3\")  # change √† \"mistral\" si tu pr√©f√®res\n\n# app/services/chat_engine.py ‚Äî rempl", "meta": {"source": "code_index", "path": "app/services/chat_engine.py"}}
{"text": "{OLLAMA_PORT}/api/generate\"\nMODEL_NAME = os.getenv(\"OLLAMA_MODEL\", \"phi3\")  # change √† \"mistral\" si tu pr√©f√®res\n\n# app/services/chat_engine.py ‚Äî remplacement de llm_local\n\n# app/services/chat_engine.py ‚Äî version robuste\n\n\n\ndef llm_local(prompt: str) -> str:\n    try:\n        # On tente d'abord une r√©ponse non stream√©e (un seul JSON)\n        payload = {\"model\": MODEL_NAME, \"prompt\": prompt, \"stream\": False}\n        r = requests.post(OLLAMA_URL, json=payload, timeout=180)\n        r.raise_for_status()\n        try:\n            j = r.json()\n            return j.get(\"response\") or \"(pas de r√©ponse du mod√®le local)\"\n        except json.JSONDecodeError:\n            # Fallback: g√©rer NDJSON (plusieurs lignes JSON)\n            pass\n\n        # Fallback NDJSON (au cas o√π le serveur ignore stream:false)\n        r = requests.post(OLLAMA_URL, json={\"model\": MODEL_NAME, \"prompt\": prompt}, stream=True, timeout=180)\n        r.raise_for_status()\n        chunks = []\n        for line in r.iter_lines(decode_", "meta": {"source": "code_index", "path": "app/services/chat_engine.py"}}
{"text": "l\": MODEL_NAME, \"prompt\": prompt}, stream=True, timeout=180)\n        r.raise_for_status()\n        chunks = []\n        for line in r.iter_lines(decode_unicode=True):\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n                # chaque chunk contient un fragment de \"response\"\n                chunks.append(obj.get(\"response\", \"\"))\n            except Exception:\n                # ignorer lignes non-JSON (s√©parateurs, etc.)\n                continue\n        text = \"\".join(chunks).strip()\n        return text or \"(r√©ponse vide)\"\n    except Exception as e:\n        return f\"(LLM local indisponible: {e})\"\n\n\n\ndef chat_with_user(message: str) -> str:\n    # ... ton code existant au-dessus\n\n    # R√©cup√®re top-k √©l√©ments de m√©moire (dicts) et ne garde que les textes\n    hits = search_memory(message, k=5)\n    memory_texts = [h.get(\"text\", \"\") for h in hits if isinstance(h, dict)]\n    context = \"\\n---\\n\".join(t for t in memory_texts if t)", "meta": {"source": "code_index", "path": "app/services/chat_engine.py"}}
{"text": "sage, k=5)\n    memory_texts = [h.get(\"text\", \"\") for h in hits if isinstance(h, dict)]\n    context = \"\\n---\\n\".join(t for t in memory_texts if t)\n\n    # Ensuite continue comme avant, en incluant `context` dans le prompt si tu veux\n    prompt = f\"Contexte:\\n{context}\\n\\nUtilisateur: {message}\\nAssistant:\"\n    reply = llm_local(prompt)\n    return reply", "meta": {"source": "code_index", "path": "app/services/chat_engine.py"}}
{"text": "import os, json, hashlib, torch\nfrom datetime import datetime\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nMEMORY_PATH = Path(\"data/memory/code_index.json\")\n\ndef hash_file(path: Path) -> str:\n    \"\"\"Calcule le hash SHA256 d‚Äôun fichier.\"\"\"\n    sha = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        while chunk := f.read(8192):\n            sha.update(chunk)\n    return sha.hexdigest()\n\ndef summarize_code_cuda(text: str, model=None) -> str:\n    \"\"\"R√©sum√© rapide du contenu du code (CPU ou GPU si dispo).\"\"\"\n    if not torch.cuda.is_available():\n        # fallback CPU simple\n        return text[:200] + \"...\" if len(text) > 200 else text\n    else:\n        # exemple simplifi√© : calcul de similarit√© ou compression vectorielle GPU\n        tokens = torch.tensor([ord(c) for c in text[:1000]], dtype=torch.float32).cuda()\n        mean_val = torch.mean(tokens).item()\n        return f\"[R√©sum√© CUDA] moyenne={mean_val:.2f}, longueur={len(text)}\"\n\ndef ingest_codebase(root_dir=\"app\"):\n    \"\"\"Scan", "meta": {"source": "code_index", "path": "app/services/code_ingest.py"}}
{"text": "rch.mean(tokens).item()\n        return f\"[R√©sum√© CUDA] moyenne={mean_val:.2f}, longueur={len(text)}\"\n\ndef ingest_codebase(root_dir=\"app\"):\n    \"\"\"Scanne le code source, d√©tecte les changements, g√©n√®re m√©moire locale.\"\"\"\n    root = Path(root_dir)\n    memory = {}\n\n    if MEMORY_PATH.exists():\n        try:\n            memory = json.load(open(MEMORY_PATH, \"r\", encoding=\"utf8\"))\n        except json.JSONDecodeError:\n            memory = {}\n\n    updated_files = []\n    for py_file in tqdm(list(root.rglob(\"*.py\")), desc=\"üß† Scan codebase\"):\n        h = hash_file(py_file)\n        if py_file.as_posix() not in memory or memory[py_file.as_posix()][\"hash\"] != h:\n            with open(py_file, \"r\", encoding=\"utf8\") as f:\n                content = f.read()\n            summary = summarize_code_cuda(content)\n            memory[py_file.as_posix()] = {\n                \"hash\": h,\n                \"summary\": summary,\n                \"last_update\": datetime.now().isoformat()\n            }\n            updated_f", "meta": {"source": "code_index", "path": "app/services/code_ingest.py"}}
{"text": "\"hash\": h,\n                \"summary\": summary,\n                \"last_update\": datetime.now().isoformat()\n            }\n            updated_files.append(py_file.as_posix())\n\n    MEMORY_PATH.parent.mkdir(parents=True, exist_ok=True)\n    with open(MEMORY_PATH, \"w\", encoding=\"utf8\") as f:\n        json.dump(memory, f, indent=2, ensure_ascii=False)\n\n    return {\"updated\": updated_files, \"total\": len(memory)}", "meta": {"source": "code_index", "path": "app/services/code_ingest.py"}}
{"text": "# app/services/code_io.py\nfrom pathlib import Path\nfrom typing import List\n\nPROJECT_ROOT = Path(__file__).resolve().parents[2]  # racine: dossier assistantia/\nALLOW_EXT = {\".py\", \".txt\", \".md\", \".json\", \".html\", \".js\", \".css\"}\n\ndef safe_path(rel_path: str) -> Path:\n    \"\"\"Retourne un Path s√©curis√©, emp√™chant la sortie de la racine projet.\"\"\"\n    p = (PROJECT_ROOT / rel_path).resolve()\n    if PROJECT_ROOT not in p.parents and p != PROJECT_ROOT:\n        raise ValueError(\"Chemin en dehors du projet interdit.\")\n    return p\n\ndef list_project_files(start: str = \"app\") -> List[str]:\n    base = safe_path(start)\n    out: List[str] = []\n    for p in base.rglob(\"*\"):\n        if p.is_file() and p.suffix.lower() in ALLOW_EXT:\n            out.append(str(p.relative_to(PROJECT_ROOT)))\n    return sorted(out)\n\ndef read_file(rel_path: str, max_bytes: int = 300_000) -> str:\n    p = safe_path(rel_path)\n    data = p.read_bytes()\n    if len(data) > max_bytes:\n        raise ValueError(\"Fichier trop volumineu", "meta": {"source": "code_index", "path": "app/services/code_io.py"}}
{"text": "300_000) -> str:\n    p = safe_path(rel_path)\n    data = p.read_bytes()\n    if len(data) > max_bytes:\n        raise ValueError(\"Fichier trop volumineux pour l'aper√ßu.\")\n    return data.decode(\"utf-8\", errors=\"replace\")", "meta": {"source": "code_index", "path": "app/services/code_io.py"}}
{"text": "Ôªøimport requests, time, threading\nfrom urllib.parse import urlparse\nfrom urllib import robotparser\nfrom bs4 import BeautifulSoup\nimport trafilatura\n\nHEADERS = {\"User-Agent\": \"AssistantDylan/1.0 (+https://example.local) Contact:dylan@example.local\"}\nREQUEST_TIMEOUT = 8\nDEFAULT_DELAY_PER_DOMAIN = 1.0\n\n_robots_cache = {}\n_last_access = {}\n_lock = threading.Lock()\n\ndef extract_with_bs4(html: str, max_chars: int = 1600) -> str:\n    soup = BeautifulSoup(html, \"html.parser\")\n    for tag in soup([\"script\",\"style\",\"noscript\"]): tag.decompose()\n    text = \" \".join(soup.get_text(separator=\" \").split())\n    return text[:max_chars]\n\ndef get_domain(url: str) -> str:\n    try:\n        return urlparse(url).netloc.lower()\n    except Exception:\n        return \"\"\n\ndef can_fetch_url(url: str, user_agent: str = HEADERS[\"User-Agent\"]) -> bool:\n    dom = get_domain(url)\n    if not dom: return False\n    base = f\"{urlparse(url).scheme}://{dom}\"\n    rp = _robots_cache.get(base)\n    if rp is None:\n        rp = ro", "meta": {"source": "code_index", "path": "app/services/extract.py"}}
{"text": "ain(url)\n    if not dom: return False\n    base = f\"{urlparse(url).scheme}://{dom}\"\n    rp = _robots_cache.get(base)\n    if rp is None:\n        rp = robotparser.RobotFileParser()\n        try:\n            rp.set_url(base + \"/robots.txt\"); rp.read()\n            _robots_cache[base] = rp\n        except Exception:\n            _robots_cache[base] = None\n            return True\n    if rp is None: return True\n    return rp.can_fetch(user_agent, url)\n\ndef polite_wait(domain: str, min_interval: float = DEFAULT_DELAY_PER_DOMAIN):\n    with _lock:\n        last = _last_access.get(domain)\n        now = time.time()\n        if last:\n            wait = min_interval - (now - last)\n            if wait > 0: time.sleep(wait)\n        _last_access[domain] = time.time()\n\ndef fetch_metadata(url: str) -> dict:\n    resp = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n    resp.raise_for_status()\n    html = resp.text\n    try:\n        txt = trafilatura.extract(html) or \"\"\n        extract = \" \".join(txt.", "meta": {"source": "code_index", "path": "app/services/extract.py"}}
{"text": "QUEST_TIMEOUT)\n    resp.raise_for_status()\n    html = resp.text\n    try:\n        txt = trafilatura.extract(html) or \"\"\n        extract = \" \".join(txt.split())[:500] if txt and len(txt.strip()) > 40 else \"\"\n    except Exception:\n        extract = \"\"\n    soup = BeautifulSoup(html, \"html.parser\")\n    title_tag = soup.find(\"title\")\n    title = title_tag.get_text(strip=True) if title_tag else None\n    meta = soup.find(\"meta\", attrs={\"name\":\"description\"}) or soup.find(\"meta\", attrs={\"property\":\"og:description\"})\n    snippet = meta.get(\"content\").strip() if meta and meta.get(\"content\") else None\n    if not snippet and not extract:\n        node = soup.find(\"p\")\n        if node and node.get_text(strip=True):\n            snippet = node.get_text(strip=True)[:300]\n    return {\"title\": title, \"snippet\": snippet, \"extract\": extract}", "meta": {"source": "code_index", "path": "app/services/extract.py"}}
{"text": "# app/services/llm_local.py\n\"\"\"\nClient simple pour LLM local via Ollama (http://127.0.0.1:11434).\n- llm_local(prompt, model=\"mistral\", temperature=0.7)\n\"\"\"\n\nimport os\nimport requests\n\nOLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://127.0.0.1:11434\")\nOLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"mistral\")  # change en \"llama3.1\" ou autre si tu veux\n\ndef llm_local(prompt: str, model: str | None = None, temperature: float = 0.7, timeout: int = 120) -> str:\n    model = model or OLLAMA_MODEL\n    try:\n        resp = requests.post(\n            f\"{OLLAMA_URL}/api/generate\",\n            json={\n                \"model\": model,\n                \"prompt\": prompt,\n                \"stream\": False,\n                \"options\": {\"temperature\": temperature}\n            },\n            timeout=timeout\n        )\n        resp.raise_for_status()\n        data = resp.json()\n        return data.get(\"response\", \"\")\n    except Exception as e:\n        return f\"[Erreur LLM local] {e}\"", "meta": {"source": "code_index", "path": "app/services/llm_local.py"}}
{"text": "# app/services/memory.py\nfrom __future__ import annotations\nimport os\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional, Tuple\nimport re\nimport math\nfrom collections import Counter\nfrom datetime import datetime\n\n# --------------------------------------------------------------------------------------\n# Paths & storage\n# --------------------------------------------------------------------------------------\nDATA_ROOT = Path(\"data\")\nMEM_DIR = DATA_ROOT / \"memory\"\nLOG_DIR = DATA_ROOT / \"logs\"\nMEM_DIR.mkdir(parents=True, exist_ok=True)\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nFREE_TEXT_PATH = MEM_DIR / \"free_text.jsonl\"         # chat / contexte libre\nPROPOSALS_PATH = MEM_DIR / \"proposals.jsonl\"         # patchs/propositions de code d‚ÄôAndy\n\n# --------------------------------------------------------------------------------------\n# Utils\n# --------------------------------------------------------------------------------------\n\ndef chunk_text(text: str, chunk:", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "------------------\n# Utils\n# --------------------------------------------------------------------------------------\n\ndef chunk_text(text: str, chunk: int = 1000, overlap: int = 150) -> List[str]:\n    \"\"\"\n    D√©coupe un texte en morceaux de taille ~chunk, avec chevauchement 'overlap'.\n    \"\"\"\n    if not text:\n        return []\n    out = []\n    i = 0\n    n = len(text)\n    while i < n:\n        j = min(i + chunk, n)\n        out.append(text[i:j])\n        if j == n:\n            break\n        i = max(0, j - overlap)\n    return out\n\ndef _normalize_meta(meta: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n    return dict(meta) if meta else {}\n\n# --------------------------------------------------------------------------------------\n# RAM store + on-disk JSONL\n# --------------------------------------------------------------------------------------\n\n# free_text items:\n# { \"text\": str, \"meta\": {\"source\": \"...\", \"path\": \"...\", ...} }\n_MEM: List[Dict[str, Any]] = []\n\ndef _load_all_free_text() -> None:", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "free_text items:\n# { \"text\": str, \"meta\": {\"source\": \"...\", \"path\": \"...\", ...} }\n_MEM: List[Dict[str, Any]] = []\n\ndef _load_all_free_text() -> None:\n    _MEM.clear()\n    if not FREE_TEXT_PATH.exists():\n        return\n    with FREE_TEXT_PATH.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n                if isinstance(obj, dict) and \"text\" in obj:\n                    obj[\"meta\"] = _normalize_meta(obj.get(\"meta\"))\n                    _MEM.append(obj)\n            except Exception:\n                continue\n\ndef _append_many_free_text(items: List[Dict[str, Any]]) -> int:\n    \"\"\"\n    Append en RAM + JSONL (sans d√©dup ici).\n    \"\"\"\n    if not items:\n        return 0\n    with FREE_TEXT_PATH.open(\"a\", encoding=\"utf-8\") as f:\n        for it in items:\n            it = {\n                \"text\": (it.get(\"text\") or \"\"),\n                \"meta\": _normalize", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "encoding=\"utf-8\") as f:\n        for it in items:\n            it = {\n                \"text\": (it.get(\"text\") or \"\"),\n                \"meta\": _normalize_meta(it.get(\"meta\"))\n            }\n            _MEM.append(it)\n            f.write(json.dumps(it, ensure_ascii=False) + \"\\n\")\n    return len(items)\n\n# Charger au premier import\n_load_all_free_text()\n\n# --------------------------------------------------------------------------------------\n# Ajout (API publique)\n# --------------------------------------------------------------------------------------\n\ndef add_many(items: List[Dict[str, Any]]) -> int:\n    \"\"\"Ajoute plusieurs items (free_text) sans d√©duplication.\"\"\"\n    if not isinstance(items, list):\n        return 0\n    return _append_many_free_text(items)\n\ndef add_many_unique(items: List[Dict[str, Any]]) -> int:\n    \"\"\"\n    Ajoute (free_text) avec d√©dup (cl√© = (text, meta.source, meta.path)).\n    \"\"\"\n    if not isinstance(items, list) or not items:\n        return 0\n\n    seen = set()\n    fo", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "avec d√©dup (cl√© = (text, meta.source, meta.path)).\n    \"\"\"\n    if not isinstance(items, list) or not items:\n        return 0\n\n    seen = set()\n    for it in _MEM:\n        t = (it.get(\"text\") or \"\").strip()\n        m = it.get(\"meta\") or {}\n        key = (t, m.get(\"source\"), m.get(\"path\"))\n        seen.add(key)\n\n    to_write: List[Dict[str, Any]] = []\n    for it in items:\n        text = (it.get(\"text\") or \"\").strip()\n        meta = _normalize_meta(it.get(\"meta\"))\n        key = (text, meta.get(\"source\"), meta.get(\"path\"))\n        if key in seen:\n            continue\n        seen.add(key)\n        to_write.append({\"text\": text, \"meta\": meta})\n\n    if not to_write:\n        return 0\n    return _append_many_free_text(to_write)\n\ndef add_to_memory(text: str, meta: Optional[Dict[str, Any]] = None) -> int:\n    \"\"\"Ajoute un seul item (compat historique).\"\"\"\n    item = {\"text\": text or \"\", \"meta\": _normalize_meta(meta)}\n    return _append_many_free_text([item])\n\n# alias utilis√© par chat_engine.py\nd", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "ue).\"\"\"\n    item = {\"text\": text or \"\", \"meta\": _normalize_meta(meta)}\n    return _append_many_free_text([item])\n\n# alias utilis√© par chat_engine.py\ndef save_text_memory(text: str, meta: Optional[Dict[str, Any]] = None) -> int:\n    return add_to_memory(text, meta)\n\n# --------------------------------------------------------------------------------------\n# Suppression cibl√©e\n# --------------------------------------------------------------------------------------\n\ndef remove_by_meta(\n    source: Optional[str] = None,\n    where: Optional[str] = None,\n    target: str = \"free_text\"\n) -> int:\n    \"\"\"\n    Supprime items selon meta.source et/ou meta.path (store 'free_text').\n    Retourne le nombre d‚Äôitems supprim√©s.\n    \"\"\"\n    if target != \"free_text\":\n        # Pas d‚Äôautre store g√©r√© ici\n        pass\n\n    if not _MEM:\n        return 0\n\n    def keep(it: Dict[str, Any]) -> bool:\n        meta = it.get(\"meta\") or {}\n        # si on filtre par source\n        if source is not None:\n            if m", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "eep(it: Dict[str, Any]) -> bool:\n        meta = it.get(\"meta\") or {}\n        # si on filtre par source\n        if source is not None:\n            if meta.get(\"source\") != source:\n                return True\n            # source match\n            if where is not None:\n                return meta.get(\"path\") != where\n            # sinon on supprime tout ce qui a cette source\n            return False\n        # si on filtre seulement par path\n        if where is not None:\n            return meta.get(\"path\") != where\n        # si aucun crit√®re, on ne supprime rien\n        return True\n\n    before = len(_MEM)\n    keep_items = [it for it in _MEM if keep(it)]\n    removed = before - len(keep_items)\n\n    if removed > 0:\n        with FREE_TEXT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n            for it in keep_items:\n                f.write(json.dumps(it, ensure_ascii=False) + \"\\n\")\n        _MEM.clear()\n        _MEM.extend(keep_items)\n\n    return removed\n\n# ------------------------------------------", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "it, ensure_ascii=False) + \"\\n\")\n        _MEM.clear()\n        _MEM.extend(keep_items)\n\n    return removed\n\n# --------------------------------------------------------------------------------------\n# Recherche (TF-IDF light)\n# --------------------------------------------------------------------------------------\n\n_WORD = re.compile(r\"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø0-9]+\")\n\ndef _tokenize(s: str) -> List[str]:\n    return [w.lower() for w in _WORD.findall(s or \"\")]\n\ndef _idf_index(corpus: List[List[str]]) -> Dict[str, float]:\n    # IDF = log(N / (1 + df)) + 1\n    N = len(corpus)\n    df = Counter()\n    for doc in corpus:\n        for w in set(doc):\n            df[w] += 1\n    return {w: math.log((N + 1) / (1 + dfw)) + 1.0 for w, dfw in df.items()}\n\ndef search_memory(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"\n    Retourne top_k items class√©s par score TF-IDF simple.\n    \"\"\"\n    if not _MEM or not query:\n        return []\n\n    docs = [_tokenize(it.get(\"text\") or \"\") for it in _MEM]\n    q_token", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "score TF-IDF simple.\n    \"\"\"\n    if not _MEM or not query:\n        return []\n\n    docs = [_tokenize(it.get(\"text\") or \"\") for it in _MEM]\n    q_tokens = _tokenize(query)\n    if not q_tokens:\n        return []\n\n    idf = _idf_index(docs)\n    q_tf = Counter(q_tokens)\n    q_vec = {w: (q_tf[w] * idf.get(w, 0.0)) for w in q_tf}\n\n    def dot(a: Dict[str, float], b: Dict[str, float]) -> float:\n        return sum(a[w] * b.get(w, 0.0) for w in a)\n\n    results: List[Tuple[float, Dict[str, Any]]] = []\n    for it, toks in zip(_MEM, docs):\n        tf = Counter(toks)\n        vec = {w: (tf[w] * idf.get(w, 0.0)) for w in tf}\n        score = dot(q_vec, vec)\n        results.append((score, it))\n\n    results.sort(key=lambda x: x[0], reverse=True)\n    out: List[Dict[str, Any]] = []\n    for s, it in results[:top_k]:\n        out.append({\"score\": s, \"text\": it.get(\"text\") or \"\", \"meta\": it.get(\"meta\") or {}})\n    return out\n\n# ----------------------------------------------------------------------------------", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "get(\"text\") or \"\", \"meta\": it.get(\"meta\") or {}})\n    return out\n\n# --------------------------------------------------------------------------------------\n# Propositions de code (patchs) ‚Äî utilis√© par self_update.py\n# --------------------------------------------------------------------------------------\n\ndef save_proposal(objective: str, patch: str, meta: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    \"\"\"\n    Sauvegarde un patch/proposition de code g√©n√©r√© par Andy.\n    √âcrit en JSONL dans PROPOSALS_PATH. Retourne l‚Äôobjet √©crit (avec ts).\n    \"\"\"\n    obj = {\n        \"ts\": datetime.utcnow().isoformat() + \"Z\",\n        \"objective\": objective or \"\",\n        \"patch\": patch or \"\",\n        \"meta\": _normalize_meta(meta),\n    }\n    with PROPOSALS_PATH.open(\"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n    return obj\n\ndef list_proposals(limit: int = 50) -> List[Dict[str, Any]]:\n    \"\"\"\n    (Optionnel) Lit les derniers patchs enregistr√©s.\n    \"\"\"", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "\\n\")\n    return obj\n\ndef list_proposals(limit: int = 50) -> List[Dict[str, Any]]:\n    \"\"\"\n    (Optionnel) Lit les derniers patchs enregistr√©s.\n    \"\"\"\n    if not PROPOSALS_PATH.exists():\n        return []\n    out: List[Dict[str, Any]] = []\n    with PROPOSALS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                out.append(json.loads(line))\n            except Exception:\n                continue\n    return out[-limit:]", "meta": {"source": "code_index", "path": "app/services/memory.py"}}
{"text": "Ôªøfrom ..database import db\nfrom ..models import SearchResult\n\ndef load_prefs():\n    conn = db()\n    row = conn.execute(\"SELECT * FROM prefs WHERE id=1\").fetchone()\n    conn.close()\n    if not row:\n        return {}\n    def split_csv(s):\n        return [x.strip().lower() for x in (s or \"\").split(\",\") if x.strip()]\n    return {\n        \"preferred_domains\": set(split_csv(row[\"preferred_domains\"])),\n        \"blocked_domains\": set(split_csv(row[\"blocked_domains\"])),\n        \"preferred_keywords\": set(split_csv(row[\"preferred_keywords\"])),\n        \"blocked_keywords\": set(split_csv(row[\"blocked_keywords\"])),\n        \"like_weight\": float(row[\"like_weight\"]),\n        \"dislike_weight\": float(row[\"dislike_weight\"]),\n        \"domain_boost\": float(row[\"domain_boost\"]),\n        \"keyword_boost\": float(row[\"keyword_boost\"]),\n        \"strict_block\": bool(row[\"strict_block\"]) if \"strict_block\" in row.keys() else False,  # <--- AJOUT\n    }\n\ndef is_hard_block(domain: str, url: str, prefs: dict, fb_counts)", "meta": {"source": "code_index", "path": "app/services/prefs.py"}}
{"text": "ow[\"strict_block\"]) if \"strict_block\" in row.keys() else False,  # <--- AJOUT\n    }\n\ndef is_hard_block(domain: str, url: str, prefs: dict, fb_counts) -> bool:\n    \"\"\"Retourne True si on DOIT cacher le r√©sultat en mode strict.\"\"\"\n    if domain in (prefs.get(\"blocked_domains\") or set()):\n        return True\n    likes_domain, dislikes_domain, likes_url, dislikes_url = fb_counts\n    # si URL dislik√©e au moins 1x -> on bloque\n    if dislikes_url.get(url, 0) > 0:\n        return True\n    # si domaine massivement dislik√© -> on bloque aussi\n    if dislikes_domain.get(domain, 0) > 0:\n        return True\n    return False\n\n\ndef get_feedback_counts():\n    conn = db(); cur = conn.cursor()\n    likes_domain = {r[\"domain\"]: r[\"c\"] for r in cur.execute(\"SELECT domain, COUNT(*) c FROM feedback WHERE label='like' GROUP BY domain\")}\n    dislikes_domain = {r[\"domain\"]: r[\"c\"] for r in cur.execute(\"SELECT domain, COUNT(*) c FROM feedback WHERE label='dislike' GROUP BY domain\")}\n    likes_url = {r[\"url\"]: r[\"", "meta": {"source": "code_index", "path": "app/services/prefs.py"}}
{"text": "domain\"]: r[\"c\"] for r in cur.execute(\"SELECT domain, COUNT(*) c FROM feedback WHERE label='dislike' GROUP BY domain\")}\n    likes_url = {r[\"url\"]: r[\"c\"] for r in cur.execute(\"SELECT url, COUNT(*) c FROM feedback WHERE label='like' GROUP BY url\")}\n    dislikes_url = {r[\"url\"]: r[\"c\"] for r in cur.execute(\"SELECT url, COUNT(*) c FROM feedback WHERE label='dislike' GROUP BY url\")}\n    conn.close()\n    return likes_domain, dislikes_domain, likes_url, dislikes_url\n\ndef score_result(item: SearchResult, query: str, prefs: dict, fb_counts, base_rank: int) -> float:\n    domain = (item.domain or \"\").lower()\n    title = (item.title or \"\")\n    snippet = (item.snippet or \"\")\n    text = f\"{title} {snippet}\".lower()\n    base = 1.0 / (base_rank + 1)\n    likes_domain, dislikes_domain, likes_url, dislikes_url = fb_counts\n    s_fb = 0.0\n    s_fb += likes_domain.get(domain, 0) * prefs[\"like_weight\"]\n    s_fb += dislikes_domain.get(domain, 0) * prefs[\"dislike_weight\"]\n    s_fb += likes_url.get(item.url, 0", "meta": {"source": "code_index", "path": "app/services/prefs.py"}}
{"text": "omain.get(domain, 0) * prefs[\"like_weight\"]\n    s_fb += dislikes_domain.get(domain, 0) * prefs[\"dislike_weight\"]\n    s_fb += likes_url.get(item.url, 0) * (prefs[\"like_weight\"] * 1.5)\n    s_fb += dislikes_url.get(item.url, 0) * (prefs[\"dislike_weight\"] * 1.5)\n    s_dom = 0.0\n    if domain in prefs[\"preferred_domains\"]: s_dom += prefs[\"domain_boost\"]\n    if domain in prefs[\"blocked_domains\"]:   s_dom -= prefs[\"domain_boost\"]\n    s_kw = 0.0\n    if any(k in text for k in prefs[\"preferred_keywords\"]): s_kw += prefs[\"keyword_boost\"]\n    if any(k in text for k in prefs[\"blocked_keywords\"]):   s_kw -= prefs[\"keyword_boost\"]\n    return base + s_fb + s_dom + s_kw", "meta": {"source": "code_index", "path": "app/services/prefs.py"}}
{"text": "Ôªøimport os, json, time, threading\nfrom typing import List, Dict, Optional\nfrom urllib.parse import urlparse\nfrom urllib import robotparser\n\nimport requests\nimport trafilatura\nfrom bs4 import BeautifulSoup\n\n# Moteurs de recherche\ntry:\n    from ddgs import DDGS\nexcept Exception:\n    from duckduckgo_search import DDGS\nfrom googlesearch import search\n\n# Similarit√© / m√©moire (GPU si dispo)\nimport torch\nfrom sentence_transformers import SentenceTransformer, util\n\nHEADERS = {\"User-Agent\": \"AssistantDylan/1.0 (+https://example.local)\"}\nREQUEST_TIMEOUT = 8\nDEFAULT_MAX_RESULTS = 50\nDEFAULT_DELAY_PER_DOMAIN = 1.0\n\nDATA_DIR = \"data\"\nos.makedirs(DATA_DIR, exist_ok=True)\nDB_PATH = os.path.join(DATA_DIR, \"user_prefs.json\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"üß† Similarit√© sur {device.upper()}\")\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n\n_lock = threading.Lock()\n_last_access: Dict[str, float] = {}\n_robots_cache: Dict[str, Optional[robotparser.RobotFileP", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "l-MiniLM-L6-v2\", device=device)\n\n_lock = threading.Lock()\n_last_access: Dict[str, float] = {}\n_robots_cache: Dict[str, Optional[robotparser.RobotFileParser]] = {}\n\nclass SearchResult:\n    def __init__(self, title=None, url=None, snippet=None, extract=None, allowed_by_robots=None, domain=None, score=0.0):\n        self.title = title\n        self.url = url\n        self.snippet = snippet\n        self.extract = extract\n        self.allowed_by_robots = allowed_by_robots\n        self.domain = domain\n        self.score = score\n\n# ---------- JSON utils ----------\ndef load_json(path: str) -> dict:\n    if not os.path.exists(path):\n        return {\"preferences\": {\"boost\": [], \"ban\": []}, \"history\": []}\n    with open(path, \"r\", encoding=\"utf8\") as f:\n        return json.load(f)\n\ndef save_json(path: str, data: dict):\n    with open(path, \"w\", encoding=\"utf8\") as f:\n        json.dump(data, f, ensure_ascii=False, indent=2)\n\n# ---------- R√©seau / parsing ----------\ndef get_domain(url: str) -> str:\n    t", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "utf8\") as f:\n        json.dump(data, f, ensure_ascii=False, indent=2)\n\n# ---------- R√©seau / parsing ----------\ndef get_domain(url: str) -> str:\n    try: return urlparse(url).netloc.lower()\n    except: return \"\"\n\ndef can_fetch_url(url: str) -> bool:\n    domain = get_domain(url)\n    if not domain: return False\n    base = f\"{urlparse(url).scheme}://{domain}\"\n    rp = _robots_cache.get(base)\n    if rp is None:\n        rp = robotparser.RobotFileParser()\n        try:\n            rp.set_url(base + \"/robots.txt\"); rp.read(); _robots_cache[base] = rp\n        except Exception:\n            _robots_cache[base] = None; return True\n    if rp is None: return True\n    return rp.can_fetch(HEADERS[\"User-Agent\"], url)\n\ndef polite_wait(domain: str, min_interval: float = DEFAULT_DELAY_PER_DOMAIN):\n    with _lock:\n        last = _last_access.get(domain); now = time.time()\n        if last:\n            wait = min_interval - (now - last)\n            if wait > 0: time.sleep(wait)\n        _last_access[domain] =", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "= time.time()\n        if last:\n            wait = min_interval - (now - last)\n            if wait > 0: time.sleep(wait)\n        _last_access[domain] = time.time()\n\ndef fetch_page_text(url: str, timeout: int = 8, max_chars: int = 1600) -> str:\n    try:\n        downloaded = trafilatura.fetch_url(url, timeout=timeout)\n        if downloaded:\n            txt = trafilatura.extract(downloaded) or \"\"\n            if txt.strip(): return txt.strip()[:max_chars]\n    except Exception: pass\n    try:\n        resp = requests.get(url, headers=HEADERS, timeout=timeout)\n        if resp.ok and resp.text:\n            txt = trafilatura.extract(resp.text) or \"\"\n            if txt.strip(): return txt.strip()[:max_chars]\n            soup = BeautifulSoup(resp.text, \"html.parser\")\n            for t in soup([\"script\",\"style\",\"noscript\"]): t.decompose()\n            return \" \".join(soup.get_text(separator=\" \").split())[:max_chars]\n    except Exception: pass\n    return \"\"\n\n# ---------- Moteurs ----------\ndef google_", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "eturn \" \".join(soup.get_text(separator=\" \").split())[:max_chars]\n    except Exception: pass\n    return \"\"\n\n# ---------- Moteurs ----------\ndef google_search_raw(query: str, max_results: int = 30):\n    try:\n        urls = list(search(query, num_results=max_results, lang=\"fr\"))\n        return [{\"title\": None, \"href\": u, \"body\": \"\"} for u in urls]\n    except Exception as e:\n        print(f\"[google] ‚ö†Ô∏è {e}\")\n        return []\n\ndef ddg_search_raw(query: str, max_results: int = 30):\n    try:\n        with DDGS() as ddgs:\n            return list(ddgs.text(query, region=\"fr-fr\", safesearch=\"moderate\", max_results=max_results))\n    except Exception as e:\n        print(f\"[ddg] ‚ö†Ô∏è {e}\")\n        return []\n\ndef hybrid_search(query: str, max_results: int = 30):\n    g = google_search_raw(query, max_results=max_results//2)\n    d = ddg_search_raw(query, max_results=max_results//2)\n    seen, out = set(), []\n    for r in g + d:\n        url = r.get(\"href\") or r.get(\"url\")\n        if url and url not in seen", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "x_results=max_results//2)\n    seen, out = set(), []\n    for r in g + d:\n        url = r.get(\"href\") or r.get(\"url\")\n        if url and url not in seen:\n            seen.add(url); out.append(r)\n    return out\n\n# ---------- Feedback / pr√©f√©rences ----------\ndef record_feedback(url: str, liked: bool):\n    data = load_json(DB_PATH)\n    prefs = data.setdefault(\"preferences\", {\"boost\": [], \"ban\": []})\n    dom = get_domain(url)\n    if liked:\n        if dom not in prefs[\"boost\"]: prefs[\"boost\"].append(dom)\n    else:\n        if dom not in prefs[\"ban\"]: prefs[\"ban\"].append(dom)\n    save_json(DB_PATH, data)\n\ndef apply_preferences(results: List[SearchResult]) -> List[SearchResult]:\n    data = load_json(DB_PATH)\n    boost = set(data.get(\"preferences\", {}).get(\"boost\", []))\n    ban = set(data.get(\"preferences\", {}).get(\"ban\", []))\n    out = []\n    for it in results:\n        d = it.domain or \"\"\n        if any(b in d for b in ban):  # filtre dur\n            continue\n        if any(b in d for b in boos", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "it in results:\n        d = it.domain or \"\"\n        if any(b in d for b in ban):  # filtre dur\n            continue\n        if any(b in d for b in boost):\n            it.score += 2.0\n        out.append(it)\n    return out\n\n# ---------- Similarit√© : filtrer ce qui ressemble aux dislikes ----------\ndef filter_by_similarity(results: List[SearchResult]) -> List[SearchResult]:\n    data = load_json(DB_PATH)\n    dislikes = [ (h.get(\"title\",\"\") + \" \" + h.get(\"snippet\",\"\")).strip()\n                 for h in data.get(\"history\", []) if h.get(\"liked\") is False ]\n    dislikes = [t for t in dislikes if t]\n    if not dislikes:\n        return results\n    # Embeddings GPU/CPU\n    dislike_vecs = model.encode(dislikes, convert_to_tensor=True)  # [N, D]\n    kept = []\n    for it in results:\n        text = ((it.title or \"\") + \" \" + (it.snippet or \"\")).strip()\n        if not text:\n            kept.append(it); continue\n        v = model.encode([text], convert_to_tensor=True)  # [1, D]\n        sim = float(util.m", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "if not text:\n            kept.append(it); continue\n        v = model.encode([text], convert_to_tensor=True)  # [1, D]\n        sim = float(util.max_sim(v1=v, v2=dislike_vecs))\n        if sim < 0.7:  # seuil de similarit√©\n            kept.append(it)\n    return kept\n\n# ---------- Rerank g√©n√©ral ----------\ndef rerank(results: List[SearchResult], query: str) -> List[SearchResult]:\n    results = apply_preferences(results)\n    results = filter_by_similarity(results)\n    # tri par score d√©croissant\n    results.sort(key=lambda x: x.score, reverse=True)\n    # Historique (best effort)\n    try:\n        data = load_json(DB_PATH)\n        for it in results[:30]:\n            data[\"history\"].append({\n                \"query\": query,\n                \"title\": it.title,\n                \"snippet\": it.snippet,\n                \"url\": it.url,\n                \"domain\": it.domain,\n                \"liked\": None,  # sera rempli si tu cliques üëç/üëé\n                \"ts\": time.time()\n            })\n        save_j", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "\"domain\": it.domain,\n                \"liked\": None,  # sera rempli si tu cliques üëç/üëé\n                \"ts\": time.time()\n            })\n        save_json(DB_PATH, data)\n    except Exception:\n        pass\n    return results\n\n# ---------- Build enrichi ----------\ndef follow_and_build(raw: List[Dict], follow=False, max_per_domain=5, delay_per_domain=1.0) -> List[SearchResult]:\n    out: List[SearchResult] = []\n    counts: Dict[str, int] = {}\n    for r in raw:\n        url = r.get(\"href\") or r.get(\"url\")\n        if not url: continue\n        dom = get_domain(url)\n        ok = can_fetch_url(url)\n        it = SearchResult(\n            title=r.get(\"title\") or url,\n            url=url,\n            snippet=(r.get(\"body\") or \"\")[:300],\n            extract=None,\n            allowed_by_robots=ok,\n            domain=dom,\n            score=0.0\n        )\n        if follow and ok:\n            if counts.get(dom, 0) < max_per_domain:\n                polite_wait(dom, delay_per_domain)\n                it.ext", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "if follow and ok:\n            if counts.get(dom, 0) < max_per_domain:\n                polite_wait(dom, delay_per_domain)\n                it.extract = fetch_page_text(url)\n                counts[dom] = counts.get(dom, 0) + 1\n        out.append(it)\n        if len(out) >= DEFAULT_MAX_RESULTS: break\n    return out", "meta": {"source": "code_index", "path": "app/services/search.py"}}
{"text": "# app/services/self_update.py\n\"\"\"\nG√©n√©ration d‚Äôam√©liorations de code par LLM local + journalisation de notes.\n- propose_code_improvement(objective) -> str\n- add_note(note) / list_notes()       -> gestion des notes affich√©es dans /self_review\n\"\"\"\n\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List\n\nfrom app.services.llm_local import llm_local\nfrom app.services.memory import save_proposal  # ‚¨ÖÔ∏è stocke les patchs g√©n√©r√©s\n\nNOTES_PATH = Path(\"data/self_notes.txt\")\nNOTES_PATH.parent.mkdir(parents=True, exist_ok=True)\n\ndef add_note(note: str) -> None:\n    \"\"\"\n    Ajoute une note de self-review (affich√©e dans /self_review).\n    \"\"\"\n    ts = datetime.utcnow().isoformat() + \"Z\"\n    with NOTES_PATH.open(\"a\", encoding=\"utf-8\") as f:\n        f.write(f\"[{ts}] {note}\\n\")\n\ndef list_notes() -> List[str]:\n    \"\"\"\n    Liste les notes existantes (format texte).\n    \"\"\"\n    if not NOTES_PATH.exists():\n        return []\n    with NOTES_PATH.open(\"r\", encoding=\"utf-8\") as f:", "meta": {"source": "code_index", "path": "app/services/self_update.py"}}
{"text": "otes existantes (format texte).\n    \"\"\"\n    if not NOTES_PATH.exists():\n        return []\n    with NOTES_PATH.open(\"r\", encoding=\"utf-8\") as f:\n        return [l.strip() for l in f if l.strip()]\n\ndef propose_code_improvement(objective: str) -> str:\n    \"\"\"\n    Demande au LLM local de proposer un patch Python (dans un bloc ```python ... ```),\n    enregistre la proposition et retourne le texte brut.\n    \"\"\"\n    prompt = (\n        \"Tu es Andy, un assistant qui propose des am√©liorations de code utiles.\\n\"\n        \"Objectif d'am√©lioration:\\n\"\n        f\"{objective}\\n\\n\"\n        \"Propose un patch **complet** en Python, clairement d√©limit√© dans un bloc Markdown:\\n\"\n        \"```python\\n# ton code ici\\n```\\n\"\n        \"Le patch doit √™tre ex√©cutable et coh√©rent.\\n\"\n    )\n    result = llm_local(prompt)\n    if not isinstance(result, str) or not result.strip():\n        result = \"[Erreur] LLM local n‚Äôa rien retourn√©.\"\n\n    # Journalise le patch dans la m√©moire\n    save_proposal(objective, result, meta", "meta": {"source": "code_index", "path": "app/services/self_update.py"}}
{"text": "strip():\n        result = \"[Erreur] LLM local n‚Äôa rien retourn√©.\"\n\n    # Journalise le patch dans la m√©moire\n    save_proposal(objective, result, meta={\"source\": \"self_propose\"})\n\n    # Ajoute une note visible dans /self_review\n    add_note(f\"Proposition g√©n√©r√©e pour: {objective}\")\n\n    return result", "meta": {"source": "code_index", "path": "app/services/self_update.py"}}
{"text": "# app/services/startup_indexer.py\nfrom __future__ import annotations\nimport json\nimport hashlib\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Tuple, Iterable\nfrom datetime import datetime\n\nfrom app.services.memory import (\n    chunk_text,\n    add_many_unique,\n    remove_by_meta,   # utilis√© pour nettoyer les anciennes entr√©es si besoin\n)\n\nDATA_ROOT = Path(\"data\")\nMEM_DIR = DATA_ROOT / \"memory\"\nMEM_DIR.mkdir(parents=True, exist_ok=True)\n\nINDEX_PATH = MEM_DIR / \"code_index.json\"\n\ndef _hash_bytes(b: bytes) -> str:\n    h = hashlib.sha1()\n    h.update(b)\n    return h.hexdigest()\n\ndef _hash_text(s: str) -> str:\n    return _hash_bytes(s.encode(\"utf-8\", errors=\"ignore\"))\n\ndef _load_index() -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Lit l'index JSON { path_posix: {hash, size, mtime, meta...}, ... }\n    \"\"\"\n    if not INDEX_PATH.exists():\n        return {}\n    try:\n        with INDEX_PATH.open(\"r\", encoding=\"utf-8\") as f:\n            obj = json.load(f)\n            if isinstance(obj", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "s():\n        return {}\n    try:\n        with INDEX_PATH.open(\"r\", encoding=\"utf-8\") as f:\n            obj = json.load(f)\n            if isinstance(obj, dict):\n                return obj\n    except Exception:\n        pass\n    return {}\n\ndef _save_index(idx: Dict[str, Dict[str, Any]]) -> None:\n    with INDEX_PATH.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(idx, f, ensure_ascii=False, indent=2)\n\ndef _iter_files(\n    start: Path,\n    allow_ext: Tuple[str, ...]\n) -> Iterable[Path]:\n    for p in start.rglob(\"*\"):\n        if p.is_file() and p.suffix.lower() in allow_ext:\n            yield p\n\ndef _scan_dir(\n    start: str | Path,\n    allow_ext: Tuple[str, ...] = (\".py\", \".js\", \".ts\", \".html\", \".css\", \".json\", \".md\", \".txt\"),\n    max_bytes: int = 300_000,\n) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Retourne un nouvel index: { path_posix: {hash, size, mtime} }\n    - paths normalis√©s en POSIX (anti flapping Windows \\ vs /)\n    - hash calcul√© sur le contenu tronqu√© √† max_bytes (suffisant p", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "hash, size, mtime} }\n    - paths normalis√©s en POSIX (anti flapping Windows \\ vs /)\n    - hash calcul√© sur le contenu tronqu√© √† max_bytes (suffisant pour d√©tecter des modifs)\n    \"\"\"\n    start_p = Path(start)\n    out: Dict[str, Dict[str, Any]] = {}\n    for p in _iter_files(start_p, allow_ext):\n        try:\n            b = p.read_bytes()\n        except Exception:\n            continue\n        b_cut = b[:max_bytes]\n        h = _hash_bytes(b_cut)\n        stat = p.stat()\n        k = p.as_posix()  # cl√© POSIX\n        out[k] = {\n            \"hash\": h,\n            \"size\": stat.st_size,\n            \"mtime\": int(stat.st_mtime),\n        }\n    return out\n\ndef _diff_index(old: Dict[str, Dict[str, Any]], new: Dict[str, Dict[str, Any]]):\n    \"\"\"\n    Calcule added/removed/changed entre deux index (cl√©s = chemins POSIX).\n    \"\"\"\n    old_keys = set(old.keys())\n    new_keys = set(new.keys())\n\n    added = sorted(new_keys - old_keys)\n    removed = sorted(old_keys - new_keys)\n\n    changed: List[str] = []", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "s())\n    new_keys = set(new.keys())\n\n    added = sorted(new_keys - old_keys)\n    removed = sorted(old_keys - new_keys)\n\n    changed: List[str] = []\n    common = old_keys & new_keys\n    for k in common:\n        if old[k].get(\"hash\") != new[k].get(\"hash\"):\n            changed.append(k)\n\n    return {\n        \"added\": added,\n        \"removed\": removed,\n        \"changed\": sorted(changed),\n    }\n\ndef _make_memory_items_for_file(path_posix: str, text: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Transforme un fichier texte en items m√©moire chunk√©s.\n    \"\"\"\n    items: List[Dict[str, Any]] = []\n    chunks = chunk_text(text, chunk=1000, overlap=150)\n    for ch in chunks:\n        items.append({\n            \"text\": ch,\n            \"meta\": {\n                \"source\": \"code_index\",\n                \"path\": path_posix\n            }\n        })\n    return items\n\ndef startup_ingest_if_changed(\n    start: str | Path = \"app\",\n    allow_ext: Tuple[str, ...] = (\".py\", \".js\", \".ts\", \".html\", \".css\", \".json\", \".m", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "ms\n\ndef startup_ingest_if_changed(\n    start: str | Path = \"app\",\n    allow_ext: Tuple[str, ...] = (\".py\", \".js\", \".ts\", \".html\", \".css\", \".json\", \".md\", \".txt\"),\n    chunk: int = 1000,\n    overlap: int = 150,\n    max_bytes: int = 300_000,\n    verbose: bool = True,\n) -> Dict[str, Any]:\n    \"\"\"\n    √Ä chaque d√©marrage:\n      - scanne 'start' et calcule un nouvel index (chemins POSIX + hash contenu)\n      - compare avec l'ancien index\n      - pour les fichiers 'added' et 'changed', (re)ingeste leur contenu en m√©moire\n      - pour les 'removed', nettoie la m√©moire correspondante\n      - sauvegarde l'index\n    Retourne {added, removed, changed, written}\n    \"\"\"\n    start_p = Path(start)\n\n    # 1) Charger index pr√©c√©dent\n    old_index = _load_index()\n\n    # 2) Scanner r√©pertoire\n    new_index = _scan_dir(start_p, allow_ext=allow_ext, max_bytes=max_bytes)\n    if verbose:\n        print(f\"[startup_indexer] scan_dir: {len(new_index)} fichiers\")\n\n    # 3) Diff\n    diff = _diff_index(old_index, ne", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "max_bytes)\n    if verbose:\n        print(f\"[startup_indexer] scan_dir: {len(new_index)} fichiers\")\n\n    # 3) Diff\n    diff = _diff_index(old_index, new_index)\n    added = diff[\"added\"]\n    removed = diff[\"removed\"]\n    changed = diff[\"changed\"]\n\n    # 4) Mise √† jour m√©moire\n    written = 0\n\n    # Nettoyage des anciens morceaux associ√©s aux fichiers supprim√©s\n    if removed:\n        # supprime tous les morceaux dont meta.source='code_index' ET meta.path=path_posix\n        count = 0\n        for rp in removed:\n            count += remove_by_meta(source=\"code_index\", where=rp, target=\"free_text\")\n        if verbose:\n            print(f\"[startup_indexer] memory cleanup: removed {count} old code_index entries\")\n\n    # Ajout / Mise √† jour des contenus\n    to_write: List[Dict[str, Any]] = []\n    for path_posix in (added + changed):\n        try:\n            text = Path(path_posix).read_text(encoding=\"utf-8\", errors=\"ignore\")\n        except Exception:\n            # En cas de lecture depuis un ch", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "text = Path(path_posix).read_text(encoding=\"utf-8\", errors=\"ignore\")\n        except Exception:\n            # En cas de lecture depuis un chemin POSIX, on recompose depuis Windows Path\n            p = Path(path_posix.replace(\"/\", \"\\\\\"))\n            try:\n                text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n            except Exception:\n                continue\n        # re-chunk avec les param√®tres donn√©s\n        chunks = chunk_text(text, chunk=chunk, overlap=overlap)\n        for ch in chunks:\n            to_write.append({\n                \"text\": ch,\n                \"meta\": {\"source\": \"code_index\", \"path\": path_posix}\n            })\n\n    if to_write:\n        written = add_many_unique(to_write)\n        if verbose:\n            print(f\"[startup_indexer] wrote {written} memory items; index will be saved to {INDEX_PATH}\")\n\n    # 5) Sauvegarder le nouvel index\n    _save_index(new_index)\n\n    out = {\n        \"added\": added,\n        \"removed\": removed,\n        \"change", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "H}\")\n\n    # 5) Sauvegarder le nouvel index\n    _save_index(new_index)\n\n    out = {\n        \"added\": added,\n        \"removed\": removed,\n        \"changed\": changed,\n        \"written\": written,\n    }\n    if verbose:\n        print(\"[startup] Ingest:\", out)\n    return out", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "# app/services/trace_logger.py\nfrom pathlib import Path\nfrom datetime import datetime\nimport json\nfrom typing import Optional, Dict, Any, List\n\nLOG_DIR = Path(\"data/logs\")\nLOG_FILE = LOG_DIR / \"generated_code_traces.jsonl\"\n\ndef _ensure_dirs():\n    try:\n        LOG_DIR.mkdir(parents=True, exist_ok=True)\n    except Exception as e:\n        raise RuntimeError(f\"Impossible de cr√©er {LOG_DIR}: {e}\")\n\ndef write_code_trace(code: str, source: str = \"self_review\", meta: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    \"\"\"\n    √âcrit imm√©diatement (synchrone) une ligne JSON dans generated_code_traces.jsonl.\n    Retourne le chemin du fichier pour v√©rification.\n    \"\"\"\n    _ensure_dirs()\n    rec = {\n        \"ts\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n        \"source\": source,\n        \"meta\": meta or {},\n        \"code\": code,\n    }\n    line = json.dumps(rec, ensure_ascii=False)\n    with LOG_FILE.open(\"a\", encoding=\"utf-8\") as f:\n        f.write(line + \"\\n\")\n    return {\"ok\": T", "meta": {"source": "code_index", "path": "app/services/trace_logger.py"}}
{"text": "}\n    line = json.dumps(rec, ensure_ascii=False)\n    with LOG_FILE.open(\"a\", encoding=\"utf-8\") as f:\n        f.write(line + \"\\n\")\n    return {\"ok\": True, \"path\": str(LOG_FILE), \"bytes\": len(line)}\n\ndef read_traces(limit: int = 100) -> List[Dict[str, Any]]:\n    _ensure_dirs()\n    if not LOG_FILE.exists():\n        return []\n    out: List[Dict[str, Any]] = []\n    with LOG_FILE.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                out.append(json.loads(line))\n            except Exception:\n                continue\n    return out[-limit:]", "meta": {"source": "code_index", "path": "app/services/trace_logger.py"}}
{"text": "# app/services/verification.py\nimport json\nimport re\nimport ast\nfrom typing import Dict, Any, Optional, Tuple\nfrom app.services.chat_engine import llm_local\n\nVERIFIER_SYSTEM = \"\"\"\nTu es un v√©rificateur de revues de code.\nBut: dire si le code propos√© atteint l'objectif sans casser l'existant.\nR√©ponds UNIQUEMENT en JSON avec les cl√©s:\n- pass (bool)\n- score (0..100)\n- reasons (array[string])\n- risks (array[string])\n- suggested_tests (array[string])\n- summary (string)\n\"\"\"\n\ndef _extract_fenced_code(text: str) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Extrait le premier bloc de code balis√© ```lang\\n...\\n``` s'il existe.\n    Retourne (lang, code) ou (None, None).\n    \"\"\"\n    m = re.search(r\"```(\\w+)?\\s*\\n(.*?)```\", text, flags=re.S|re.M)\n    if not m:\n        return None, None\n    lang = (m.group(1) or \"\").strip().lower()\n    code = m.group(2)\n    return lang, code\n\ndef _local_syntax_check(lang: Optional[str], code: Optional[str]) -> Dict[str, Any]:\n    \"\"\"\n    V√©rifications locales", "meta": {"source": "code_index", "path": "app/services/verification.py"}}
{"text": "group(2)\n    return lang, code\n\ndef _local_syntax_check(lang: Optional[str], code: Optional[str]) -> Dict[str, Any]:\n    \"\"\"\n    V√©rifications locales non bloquantes : syntaxe Python/JSON/JS simple.\n    \"\"\"\n    if not code:\n        return {\"syntax_ok\": None, \"syntax_error\": None, \"lang\": lang}\n\n    if lang in (\"python\", \"py\"):\n        try:\n            ast.parse(code)\n            return {\"syntax_ok\": True, \"syntax_error\": None, \"lang\": lang}\n        except SyntaxError as e:\n            return {\"syntax_ok\": False, \"syntax_error\": str(e), \"lang\": lang}\n\n    if lang == \"json\":\n        try:\n            json.loads(code)\n            return {\"syntax_ok\": True, \"syntax_error\": None, \"lang\": lang}\n        except Exception as e:\n            return {\"syntax_ok\": False, \"syntax_error\": str(e), \"lang\": lang}\n\n    # Basique pour JS/TS: on ne parse pas, on note juste non v√©rifi√©\n    if lang in (\"js\", \"javascript\", \"ts\", \"typescript\"):\n        return {\"syntax_ok\": None, \"syntax_error\": None, \"lang\": la", "meta": {"source": "code_index", "path": "app/services/verification.py"}}
{"text": "on note juste non v√©rifi√©\n    if lang in (\"js\", \"javascript\", \"ts\", \"typescript\"):\n        return {\"syntax_ok\": None, \"syntax_error\": None, \"lang\": lang}\n\n    return {\"syntax_ok\": None, \"syntax_error\": None, \"lang\": lang}\n\ndef build_verifier_prompt(objective: str, proposal: str, context: Optional[str] = None) -> str:\n    ctx = f\"\\nContexte:\\n{context}\\n\" if context else \"\"\n    return (\n        f\"{VERIFIER_SYSTEM}\\n\"\n        f\"Objectif:\\n{objective}\\n\"\n        f\"{ctx}\"\n        f\"Proposition:\\n{proposal}\\n\\n\"\n        \"R√©ponds en JSON uniquement.\"\n    )\n\ndef verify_patch(objective: str, proposal: str, context: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"\n    1) V√©rifie localement la syntaxe du bloc de code (si pr√©sent).\n    2) Demande un verdict structur√© au LLM local.\n    \"\"\"\n    lang, code = _extract_fenced_code(proposal)\n    syntax = _local_syntax_check(lang, code)\n\n    prompt = build_verifier_prompt(objective, proposal, context=context)\n    raw = llm_local(prompt)\n\n    try:", "meta": {"source": "code_index", "path": "app/services/verification.py"}}
{"text": "_local_syntax_check(lang, code)\n\n    prompt = build_verifier_prompt(objective, proposal, context=context)\n    raw = llm_local(prompt)\n\n    try:\n        verdict = json.loads(raw)\n        # garde au moins les cl√©s attendues\n        for k in [\"pass\", \"score\", \"reasons\", \"risks\", \"suggested_tests\", \"summary\"]:\n            verdict.setdefault(k, None)\n    except Exception:\n        verdict = {\n            \"pass\": False,\n            \"score\": 0,\n            \"reasons\": [\"R√©ponse non-JSON du LLM.\"],\n            \"risks\": [],\n            \"suggested_tests\": [],\n            \"summary\": (raw or \"\").strip()[:2000],\n        }\n\n    return {\n        \"syntax\": syntax,   # ex: {\"syntax_ok\": True/False/None, \"syntax_error\": \"...\", \"lang\": \"python\"}\n        \"verdict\": verdict  # JSON du mod√®le (pass/score/reasons/risks/suggested_tests/summary)\n    }", "meta": {"source": "code_index", "path": "app/services/verification.py"}}
{"text": "Ôªø# app/main.py\nimport os\nfrom pathlib import Path\nfrom fastapi import FastAPI\nfrom fastapi.responses import HTMLResponse\n\n# --- Import des routeurs ---\nfrom app.routers.deep_search import router as search_router\nfrom app.routers.chat import router as chat_router\nfrom app.routers.self_update_router import router as self_router\nfrom app.routers.code_review import router as code_review_router\nfrom app.routers.trace import router as trace_router\nfrom app.routers.patch_router import router as patch_router\n\n# --- Services ---\nfrom app.services.startup_indexer import startup_ingest_if_changed\nfrom app.services.code_ingest import ingest_codebase\n\n# --- Initialisation m√©moire ---\n\n\n# --- Application principale ---\napp = FastAPI(title=\"Assistant IA √©volutif (GPU) de Dylan\")\n\n# --- Inclusion des routeurs ---\napp.include_router(patch_router)\napp.include_router(search_router)\napp.include_router(chat_router)\napp.include_router(self_router)\napp.include_router(code_review_router)\napp.include_router(tr", "meta": {"source": "code_index", "path": "app/main.py"}}
{"text": "ude_router(search_router)\napp.include_router(chat_router)\napp.include_router(self_router)\napp.include_router(code_review_router)\napp.include_router(trace_router)\n\n# --- Auto-ingestion au d√©marrage ---\n@app.on_event(\"startup\")\ndef _auto_ingest_on_start():\n    \"\"\"\n    √Ä chaque d√©marrage, Andy scanne les fichiers et m√©morise les changements.\n    D√©sactiver avec ENV: ANDY_AUTO_INGEST=0\n    \"\"\"\n    if os.getenv(\"ANDY_AUTO_INGEST\", \"1\") != \"1\":\n        print(\"[startup] Auto-ingest d√©sactiv√© (ANDY_AUTO_INGEST=0).\")\n        return\n\n    res = startup_ingest_if_changed(\n        start=\"app\",\n        allow_ext=(\".py\", \".js\", \".ts\", \".html\", \".css\", \".json\", \".md\", \".txt\"),\n        chunk=1000,\n        overlap=150,\n        max_bytes=300_000,\n        verbose=True\n    )\n    print(\"[startup] Ingest:\", res)\n\n# --- Page d'accueil ---\n@app.get(\"/\")\ndef root():\n    return {\n        \"ok\": True,\n        \"msg\": \"Assistant IA (GPU) ‚Äî pr√™t.\",\n        \"routes\": [r.path for r in app.router.routes],\n        \"ui\":", "meta": {"source": "code_index", "path": "app/main.py"}}
{"text": "):\n    return {\n        \"ok\": True,\n        \"msg\": \"Assistant IA (GPU) ‚Äî pr√™t.\",\n        \"routes\": [r.path for r in app.router.routes],\n        \"ui\": [\"/ui\", \"/chat_ui\", \"/profile\", \"/self_review\"]\n    }\n\n# --- Page Self Review (interface d‚Äôam√©lioration) ---\n@app.get(\"/self_review\", response_class=HTMLResponse)\ndef get_self_review():\n    html_path = Path(\"app/static/self_review.html\")\n    if not html_path.exists():\n        return HTMLResponse(\"<h1>‚ùå Fichier self_review.html introuvable</h1>\", status_code=404)\n\n    # üîÑ Forcer √† relire sans cache navigateur\n    html = html_path.read_text(encoding=\"utf8\")\n    html = html.replace(\n        \"</head>\",\n        '<meta http-equiv=\"Cache-Control\" content=\"no-cache, no-store, must-revalidate\" /></head>'\n    )\n    return HTMLResponse(content=html)", "meta": {"source": "code_index", "path": "app/main.py"}}
{"text": "# app/services/startup_indexer.py\nfrom __future__ import annotations\nimport json\nimport hashlib\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Tuple, Iterable\nfrom datetime import datetime\n\nfrom app.services.memory import (\n    chunk_text,\n    add_many_unique,\n    remove_by_meta,   # utilis√© pour nettoyer les anciennes entr√©es si besoin\n)\n\nDATA_ROOT = Path(\"data\")\nMEM_DIR = DATA_ROOT / \"memory\"\nMEM_DIR.mkdir(parents=True, exist_ok=True)\n\nINDEX_PATH = MEM_DIR / \"code_index.json\"\n\ndef _hash_bytes(b: bytes) -> str:\n    h = hashlib.sha1()\n    h.update(b)\n    return h.hexdigest()\n\ndef _hash_text(s: str) -> str:\n    return _hash_bytes(s.encode(\"utf-8\", errors=\"ignore\"))\n\ndef _load_index() -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Lit l'index JSON et normalise toutes les cl√©s en POSIX (anti \\ vs /).\n    Si l'ancien fichier contenait des cl√©s en backslash, on les convertit ici.\n    \"\"\"\n    if not INDEX_PATH.exists():\n        return {}\n\n    try:\n        with INDEX_PATH.open(\"r\", e", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "t des cl√©s en backslash, on les convertit ici.\n    \"\"\"\n    if not INDEX_PATH.exists():\n        return {}\n\n    try:\n        with INDEX_PATH.open(\"r\", encoding=\"utf-8\") as f:\n            raw = json.load(f)\n    except Exception:\n        return {}\n\n    if not isinstance(raw, dict):\n        return {}\n\n    migrated: Dict[str, Dict[str, Any]] = {}\n    for k, v in raw.items():\n        # normalise en / (POSIX)\n        k_posix = k.replace(\"\\\\\", \"/\")\n        # si collision improbable, on garde la derni√®re (ou on pourrait fusionner)\n        migrated[k_posix] = v\n\n    # si on a chang√© quelque chose, r√©√©crit l‚Äôindex migr√© une fois\n    if set(migrated.keys()) != set(raw.keys()):\n        try:\n            with INDEX_PATH.open(\"w\", encoding=\"utf-8\") as f:\n                json.dump(migrated, f, ensure_ascii=False, indent=2)\n        except Exception:\n            pass\n\n    return migrated\n\n\ndef _save_index(idx: Dict[str, Dict[str, Any]]) -> None:\n    with INDEX_PATH.open(\"w\", encoding=\"utf-8\") as f:", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "pass\n\n    return migrated\n\n\ndef _save_index(idx: Dict[str, Dict[str, Any]]) -> None:\n    with INDEX_PATH.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(idx, f, ensure_ascii=False, indent=2)\n\ndef _iter_files(\n    start: Path,\n    allow_ext: Tuple[str, ...]\n) -> Iterable[Path]:\n    for p in start.rglob(\"*\"):\n        if p.is_file() and p.suffix.lower() in allow_ext:\n            yield p\n\ndef _scan_dir(\n    start: str | Path,\n    allow_ext: Tuple[str, ...] = (\".py\", \".js\", \".ts\", \".html\", \".css\", \".json\", \".md\", \".txt\"),\n    max_bytes: int = 300_000,\n) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Retourne un nouvel index: { path_posix: {hash, size, mtime} }\n    - paths normalis√©s en POSIX (anti flapping Windows \\ vs /)\n    - hash calcul√© sur le contenu tronqu√© √† max_bytes (suffisant pour d√©tecter des modifs)\n    \"\"\"\n    start_p = Path(start)\n    out: Dict[str, Dict[str, Any]] = {}\n    for p in _iter_files(start_p, allow_ext):\n        try:\n            b = p.read_bytes()\n        except", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "tart)\n    out: Dict[str, Dict[str, Any]] = {}\n    for p in _iter_files(start_p, allow_ext):\n        try:\n            b = p.read_bytes()\n        except Exception:\n            continue\n        b_cut = b[:max_bytes]\n        h = _hash_bytes(b_cut)\n        stat = p.stat()\n        k = p.as_posix()  # cl√© POSIX\n        out[k] = {\n            \"hash\": h,\n            \"size\": stat.st_size,\n            \"mtime\": int(stat.st_mtime),\n        }\n    return out\n\ndef _diff_index(old: Dict[str, Dict[str, Any]], new: Dict[str, Dict[str, Any]]):\n    \"\"\"\n    Calcule added/removed/changed entre deux index (cl√©s = chemins POSIX).\n    \"\"\"\n    old_keys = set(old.keys())\n    new_keys = set(new.keys())\n\n    added = sorted(new_keys - old_keys)\n    removed = sorted(old_keys - new_keys)\n\n    changed: List[str] = []\n    common = old_keys & new_keys\n    for k in common:\n        if old[k].get(\"hash\") != new[k].get(\"hash\"):\n            changed.append(k)\n\n    return {\n        \"added\": added,\n        \"removed\": removed,", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "if old[k].get(\"hash\") != new[k].get(\"hash\"):\n            changed.append(k)\n\n    return {\n        \"added\": added,\n        \"removed\": removed,\n        \"changed\": sorted(changed),\n    }\n\ndef _make_memory_items_for_file(path_posix: str, text: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Transforme un fichier texte en items m√©moire chunk√©s.\n    \"\"\"\n    items: List[Dict[str, Any]] = []\n    chunks = chunk_text(text, chunk=1000, overlap=150)\n    for ch in chunks:\n        items.append({\n            \"text\": ch,\n            \"meta\": {\n                \"source\": \"code_index\",\n                \"path\": path_posix\n            }\n        })\n    return items\n\ndef startup_ingest_if_changed(\n    start: str | Path = \"app\",\n    allow_ext: Tuple[str, ...] = (\".py\", \".js\", \".ts\", \".html\", \".css\", \".json\", \".md\", \".txt\"),\n    chunk: int = 1000,\n    overlap: int = 150,\n    max_bytes: int = 300_000,\n    verbose: bool = True,\n) -> Dict[str, Any]:\n    \"\"\"\n    √Ä chaque d√©marrage:\n      - scanne 'start' et calcule", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "= 150,\n    max_bytes: int = 300_000,\n    verbose: bool = True,\n) -> Dict[str, Any]:\n    \"\"\"\n    √Ä chaque d√©marrage:\n      - scanne 'start' et calcule un nouvel index (chemins POSIX + hash contenu)\n      - compare avec l'ancien index\n      - pour les fichiers 'added' et 'changed', (re)ingeste leur contenu en m√©moire\n      - pour les 'removed', nettoie la m√©moire correspondante\n      - sauvegarde l'index\n    Retourne {added, removed, changed, written}\n    \"\"\"\n    start_p = Path(start)\n\n    # 1) Charger index pr√©c√©dent\n    old_index = _load_index()\n\n    # 2) Scanner r√©pertoire\n    new_index = _scan_dir(start_p, allow_ext=allow_ext, max_bytes=max_bytes)\n    if verbose:\n        print(f\"[startup_indexer] scan_dir: {len(new_index)} fichiers\")\n\n    # 3) Diff\n    diff = _diff_index(old_index, new_index)\n    added = diff[\"added\"]\n    removed = diff[\"removed\"]\n    changed = diff[\"changed\"]\n\n    # 4) Mise √† jour m√©moire\n    written = 0\n\n    # Nettoyage des anciens morceaux associ√©s aux fichiers su", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "[\"removed\"]\n    changed = diff[\"changed\"]\n\n    # 4) Mise √† jour m√©moire\n    written = 0\n\n    # Nettoyage des anciens morceaux associ√©s aux fichiers supprim√©s\n    if removed:\n        # supprime tous les morceaux dont meta.source='code_index' ET meta.path=path_posix\n        count = 0\n        for rp in removed:\n            count += remove_by_meta(source=\"code_index\", where=rp, target=\"free_text\")\n        if verbose:\n            print(f\"[startup_indexer] memory cleanup: removed {count} old code_index entries\")\n\n    # Ajout / Mise √† jour des contenus\n    to_write: List[Dict[str, Any]] = []\n    for path_posix in (added + changed):\n        try:\n            text = Path(path_posix).read_text(encoding=\"utf-8\", errors=\"ignore\")\n        except Exception:\n            # En cas de lecture depuis un chemin POSIX, on recompose depuis Windows Path\n            p = Path(path_posix.replace(\"/\", \"\\\\\"))\n            try:\n                text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n            except", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "p = Path(path_posix.replace(\"/\", \"\\\\\"))\n            try:\n                text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n            except Exception:\n                continue\n        # re-chunk avec les param√®tres donn√©s\n        chunks = chunk_text(text, chunk=chunk, overlap=overlap)\n        for ch in chunks:\n            to_write.append({\n                \"text\": ch,\n                \"meta\": {\"source\": \"code_index\", \"path\": path_posix}\n            })\n\n    if to_write:\n        written = add_many_unique(to_write)\n        if verbose:\n            print(f\"[startup_indexer] wrote {written} memory items; index will be saved to {INDEX_PATH}\")\n\n    # 5) Sauvegarder le nouvel index\n    _save_index(new_index)\n\n    out = {\n        \"added\": added,\n        \"removed\": removed,\n        \"changed\": changed,\n        \"written\": written,\n    }\n    if verbose:\n        print(\"[startup] Ingest:\", out)\n    return out", "meta": {"source": "code_index", "path": "app/services/startup_indexer.py"}}
{"text": "Ôªøimport os\nfrom fastapi import FastAPI\nfrom app.routers.deep_search import router as search_router\nfrom app.routers.chat import router as chat_router\nfrom app.routers.self_update_router import router as self_router\nfrom app.routers.code_review import router as code_review_router\nfrom app.routers.trace import router as trace_router\nfrom app.routers.patch_router import router as patch_router\nfrom app.services.startup_indexer import startup_ingest_if_changed\n\napp = FastAPI(title=\"Assistant IA √©volutif (GPU) de Dylan\")\n\n# Routers\napp.include_router(patch_router)\napp.include_router(search_router)\napp.include_router(chat_router)\napp.include_router(self_router)\napp.include_router(code_review_router)\napp.include_router(trace_router)\n\n_started_once = False\n\n@app.on_event(\"startup\")\ndef _auto_ingest_on_start():\n    global _started_once\n    if _started_once:\n        return            # garde-fou si import double\n    _started_once = True\n\n    if os.getenv(\"ANDY_AUTO_INGEST\", \"1\") != \"1\":\n        p", "meta": {"source": "code_index", "path": "app/main.py"}}
{"text": "ted_once:\n        return            # garde-fou si import double\n    _started_once = True\n\n    if os.getenv(\"ANDY_AUTO_INGEST\", \"1\") != \"1\":\n        print(\"[startup] Auto-ingest d√©sactiv√© (ANDY_AUTO_INGEST=0).\")\n        return\n\n    res = startup_ingest_if_changed(\n        start=\"app\",\n        allow_ext=(\".py\",\".js\",\".ts\",\".html\",\".css\",\".json\",\".md\",\".txt\"),\n        chunk=1000,\n        overlap=150,\n        max_bytes=300_000,\n        verbose=True\n    )\n    print(\"[startup] Ingest:\", res)\n\n@app.get(\"/\")\ndef root():\n    return {\n        \"ok\": True,\n        \"msg\": \"Assistant IA (GPU) ‚Äî pr√™t.\",\n        \"routes\": [r.path for r in app.router.routes],\n        \"ui\": [\"/ui\", \"/chat_ui\", \"/profile\", \"/self_review\"]\n    }", "meta": {"source": "code_index", "path": "app/main.py"}}
